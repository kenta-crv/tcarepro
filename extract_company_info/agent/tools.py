"""ãƒãƒ«ãƒãƒ„ãƒ¼ãƒ«ç”¨ã®é–¢æ•°å®šç¾©ã¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼.

Google Generative APIã®FunctionDeclarationå½¢å¼ã§é–¢æ•°ãƒ„ãƒ¼ãƒ«ã‚’å®šç¾©ã—ã€
LLMã‹ã‚‰ã®å‘¼ã³å‡ºã—ã‚’å‡¦ç†ã™ã‚‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import json
from typing import Any, Dict, Optional
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
from google.genai import types as genai_types

from utils.crawl4ai_util import crawl_markdown
from utils.logger import get_logger
from utils.net import _check_single_url
from utils.validator import (
    validate_address_format,
    validate_company_format,
    validate_tel_format,
    valid_business,
    valid_genre,
)

logger = get_logger()


def _dict_to_schema(schema_dict: Dict[str, Any]) -> genai_types.Schema:
    """è¾æ›¸å½¢å¼ã®JSON Schemaã‚’genai Schemaã«å¤‰æ›."""
    return genai_types.Schema.model_validate(schema_dict)


def get_check_url_accessibility_declaration() -> genai_types.FunctionDeclaration:
    """URLåˆ°é”å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯é–¢æ•°ã®FunctionDeclarationã‚’è¿”ã™."""
    schema_dict = {
        "type": "object",
        "properties": {
            "url": {
                "type": "string",
                "description": "ãƒã‚§ãƒƒã‚¯ã™ã‚‹URLï¼ˆä¾‹: https://example.comï¼‰",
            },
        },
        "required": ["url"],
    }
    return genai_types.FunctionDeclaration(
        name="check_url_accessibility",
        description=(
            "æŒ‡å®šã•ã‚ŒãŸURLãŒã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚"
            "URLã«åˆ°é”ã§ãã‚‹å ´åˆã¯æœ€çµ‚URLï¼ˆãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå¾Œã®URLï¼‰ã‚’è¿”ã—ã€"
            "åˆ°é”ã§ããªã„å ´åˆã¯Noneã‚’è¿”ã—ã¾ã™ã€‚"
        ),
        parameters=_dict_to_schema(schema_dict),
    )


def get_crawl_website_declaration() -> genai_types.FunctionDeclaration:
    """ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚¯ãƒ­ãƒ¼ãƒ«é–¢æ•°ã®FunctionDeclarationã‚’è¿”ã™."""
    schema_dict = {
        "type": "object",
        "properties": {
            "url": {
                "type": "string",
                "description": "ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®URL",
            },
            "timeout": {
                "type": "integer",
                "description": "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ç§’ï¼‰",
                "default": 10,
            },
        },
        "required": ["url"],
    }
    return genai_types.FunctionDeclaration(
        name="crawl_website",
        description=(
            "æŒ‡å®šã•ã‚ŒãŸURLã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€Markdownå½¢å¼ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã¾ã™ã€‚"
            "ã‚¯ãƒ­ãƒ¼ãƒ«ã«å¤±æ•—ã—ãŸå ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã—ã¾ã™ã€‚"
        ),
        parameters=_dict_to_schema(schema_dict),
    )


def get_crawl_footer_links_declaration() -> genai_types.FunctionDeclaration:
    """ãƒ•ãƒƒã‚¿ãƒ¼ã¨é–¢é€£ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹FunctionDeclarationã‚’è¿”ã™."""
    schema_dict = {
        "type": "object",
        "properties": {
            "url": {
                "type": "string",
                "description": "ãƒ•ãƒƒã‚¿ãƒ¼ã‚’è§£æã™ã‚‹ãƒ™ãƒ¼ã‚¹URL",
            },
            "max_links": {
                "type": "integer",
                "description": "ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹ãƒ•ãƒƒã‚¿ãƒ¼ãƒªãƒ³ã‚¯ã®æœ€å¤§æ•°ï¼ˆ1-10ï¼‰",
                "default": 5,
            },
            "timeout": {
                "type": "integer",
                "description": "ãƒ™ãƒ¼ã‚¹URLå–å¾—æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°",
                "default": 10,
            },
            "link_timeout": {
                "type": "integer",
                "description": "ãƒ•ãƒƒã‚¿ãƒ¼ãƒªãƒ³ã‚¯ã‚¯ãƒ­ãƒ¼ãƒ«æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°",
                "default": 10,
            },
        },
        "required": ["url"],
    }
    return genai_types.FunctionDeclaration(
        name="crawl_footer_links",
        description=(
            "ãƒšãƒ¼ã‚¸ã®ãƒ•ãƒƒã‚¿ãƒ¼ã‚’å–å¾—ã—ã€é‡è¦ãã†ãªãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦Markdownã‚’è¿”ã—ã¾ã™ã€‚"
            "å•ã„åˆã‚ã›ãƒ»ä¼šç¤¾æ¦‚è¦ãªã©ãƒ•ãƒƒã‚¿ãƒ¼ã«ã—ã‹ãªã„æƒ…å ±ã®åé›†ã«åˆ©ç”¨ã—ã¦ãã ã•ã„ã€‚"
        ),
        parameters=_dict_to_schema(schema_dict),
    )


def get_validate_company_info_declaration() -> genai_types.FunctionDeclaration:
    """ä¼šç¤¾æƒ…å ±ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–¢æ•°ã®FunctionDeclarationã‚’è¿”ã™."""
    schema_dict = {
        "type": "object",
        "properties": {
            "company": {
                "type": "string",
                "description": "ä¼šç¤¾åï¼ˆã€Œæ ªå¼ä¼šç¤¾ã€ã€Œæœ‰é™ä¼šç¤¾ã€ãªã©ã‚’å«ã‚€å¿…è¦ãŒã‚ã‚‹ï¼‰",
            },
            "tel": {
                "type": "string",
                "description": "é›»è©±ç•ªå·ï¼ˆåŠè§’æ•°å­—ã¨ãƒã‚¤ãƒ•ãƒ³ã®ã¿ã€ãƒã‚¤ãƒ•ãƒ³ã‚’å«ã‚€å¿…è¦ãŒã‚ã‚‹ï¼‰",
            },
            "address": {
                "type": "string",
                "description": "ä½æ‰€ï¼ˆã€Œéƒ½ã€ã€Œé“ã€ã€Œåºœã€ã€ŒçœŒã€ã®ã„ãšã‚Œã‹ã‚’å«ã‚€å¿…è¦ãŒã‚ã‚‹ï¼‰",
            },
        },
        "required": [],
    }
    return genai_types.FunctionDeclaration(
        name="validate_company_info",
        description=(
            "æŠ½å‡ºã—ãŸä¼šç¤¾æƒ…å ±ã®å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚"
            "æ¤œè¨¼çµæœã¨ã€ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯ä¿®æ­£æ¡ˆã‚’è¿”ã—ã¾ã™ã€‚"
        ),
        parameters=_dict_to_schema(schema_dict),
    )


def get_report_url_scores_declaration() -> genai_types.FunctionDeclaration:
    """URLã‚¹ã‚³ã‚¢ã‚’æ§‹é€ åŒ–ã—ã¦è¿”ã™ãŸã‚ã®FunctionDeclaration."""
    schema_dict = {
        "type": "object",
        "properties": {
            "urls": {
                "type": "array",
                "description": "URLã‚¹ã‚³ã‚¢ã®ãƒªã‚¹ãƒˆ",
                "items": {
                    "type": "object",
                    "properties": {
                        "url": {"type": "string", "description": "URL"},
                        "score": {
                            "type": "number",
                            "description": "é–¢é€£åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰",
                        },
                    },
                    "required": ["url", "score"],
                },
            }
        },
        "required": ["urls"],
    }
    return genai_types.FunctionDeclaration(
        name="report_url_scores",
        description="URLå€™è£œã¨å„ã‚¹ã‚³ã‚¢ã‚’æ§‹é€ åŒ–ã—ã¦è¿”ã—ã¾ã™ã€‚",
        parameters=_dict_to_schema(schema_dict),
    )


def get_report_company_info_declaration() -> genai_types.FunctionDeclaration:
    """ä¼šç¤¾æƒ…å ±ã‚’æ§‹é€ åŒ–ã—ã¦è¿”ã™ãŸã‚ã®FunctionDeclaration."""
    schema_dict = {
        "type": "object",
        "properties": {
            "company": {"type": "string", "description": "ä¼šç¤¾å"},
            "tel": {"type": "string", "description": "é›»è©±ç•ªå·"},
            "address": {"type": "string", "description": "ä½æ‰€"},
            "first_name": {"type": "string", "description": "ä»£è¡¨è€…ãƒ»æ‹…å½“è€…å"},
            "url": {"type": "string", "description": "å…¬å¼URL"},
            "contact_url": {"type": "string", "description": "å•ã„åˆã‚ã›URL"},
        },
        "required": ["company", "tel", "address", "url"],
    }
    return genai_types.FunctionDeclaration(
        name="report_company_info",
        description="æŠ½å‡ºã—ãŸä¼šç¤¾æƒ…å ±ã‚’æ§‹é€ åŒ–ã—ã¦è¿”ã—ã¾ã™ã€‚",
        parameters=_dict_to_schema(schema_dict),
    )


def handle_check_url_accessibility(args: Dict[str, Any]) -> Dict[str, Any]:
    """check_url_accessibilityé–¢æ•°ã®å‘¼ã³å‡ºã—ã‚’å‡¦ç†."""
    url = args.get("url", "")
    if not url:
        return {"accessible": False, "final_url": None, "error": "URLãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"}

    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
        timeout = 10
        result = _check_single_url(url, timeout, headers)
        if result:
            logger.debug(f"  âœ… check_url_accessibility: {url} -> {result}")
            return {"accessible": True, "final_url": result}
        else:
            logger.debug(f"  âŒ check_url_accessibility: {url} - åˆ°é”ä¸å¯")
            return {"accessible": False, "final_url": None, "error": "URLã«åˆ°é”ã§ãã¾ã›ã‚“ã§ã—ãŸ"}
    except Exception as e:
        logger.error(f"  âŒ check_url_accessibility ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
        return {"accessible": False, "final_url": None, "error": f"ã‚¨ãƒ©ãƒ¼: {str(e)[:200]}"}


def handle_crawl_website(args: Dict[str, Any]) -> Dict[str, Any]:
    """crawl_websiteé–¢æ•°ã®å‘¼ã³å‡ºã—ã‚’å‡¦ç†."""
    url = args.get("url", "")
    timeout = args.get("timeout", 10)

    if not url:
        return {"success": False, "content": "", "error": "URLãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"}

    try:
        logger.debug(f"  ğŸ•·ï¸ crawl_website: {url} (timeout={timeout}ç§’)")
        content = crawl_markdown(url, depth=0, timeout=timeout)
        if content:
            logger.debug(f"  âœ… crawl_website: {len(content)}æ–‡å­—å–å¾—æˆåŠŸ")
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒé•·ã™ãã‚‹å ´åˆã¯å…ˆé ­éƒ¨åˆ†ã®ã¿è¿”ã™
            max_length = 50000  # 50,000æ–‡å­—ã«åˆ¶é™
            if len(content) > max_length:
                content = content[:max_length] + "\n\n[... ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒé•·ã„ãŸã‚é€”ä¸­ã§åˆ‡ã‚‰ã‚Œã¦ã„ã¾ã™ ...]"
            return {"success": True, "content": content, "length": len(content)}
        else:
            logger.warning(f"  âš ï¸ crawl_website: {url} - ã‚¯ãƒ­ãƒ¼ãƒ«å¤±æ•—ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            return {"success": False, "content": "", "error": "ã‚¯ãƒ­ãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸ"}
    except Exception as e:
        logger.error(f"  âŒ crawl_website ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
        return {"success": False, "content": "", "error": f"ã‚¨ãƒ©ãƒ¼: {str(e)[:200]}"}


def handle_crawl_footer_links(args: Dict[str, Any]) -> Dict[str, Any]:
    """ãƒ•ãƒƒã‚¿ãƒ¼ã¨ãã®ãƒªãƒ³ã‚¯å…ˆã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦è¿”ã™."""
    url = (args.get("url") or "").strip()
    if not url:
        return {"success": False, "footer_text": "", "links": [], "error": "URLãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"}

    max_links = int(args.get("max_links", 5) or 5)
    max_links = max(1, min(max_links, 10))
    timeout = int(args.get("timeout", 10) or 10)
    link_timeout = int(args.get("link_timeout", 10) or 10)

    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    try:
        resp = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True, verify=False)
        resp.raise_for_status()
    except requests.RequestException as e:  # noqa: PERF203
        logger.error(f"  âŒ crawl_footer_links: ãƒ™ãƒ¼ã‚¹URLå–å¾—å¤±æ•—: {type(e).__name__}: {str(e)[:200]}")
        return {
            "success": False,
            "footer_text": "",
            "links": [],
            "error": f"ãƒ™ãƒ¼ã‚¹URLã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)[:200]}",
        }

    soup = BeautifulSoup(resp.text, "html.parser")
    footer = soup.find("footer")
    footer_text = footer.get_text(separator="\n", strip=True) if footer else ""

    if not footer:
        logger.warning("  âš ï¸ crawl_footer_links: <footer>è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    base_url = resp.url or url
    anchors = footer.find_all("a", href=True) if footer else []

    links = []
    seen = set()
    for anchor in anchors:
        if len(links) >= max_links:
            break
        href = anchor.get("href", "").strip()
        if not href:
            continue
        absolute_url = urljoin(base_url, href)
        parsed = urlparse(absolute_url)
        if parsed.scheme not in {"http", "https"}:
            continue
        if absolute_url.lower().startswith(("mailto:", "tel:")):
            continue
        if absolute_url in seen:
            continue
        seen.add(absolute_url)

        anchor_text = anchor.get_text(" ", strip=True)
        try:
            markdown = crawl_markdown(absolute_url, timeout=link_timeout)
        except Exception as e:  # noqa: BLE001
            logger.warning(f"  âš ï¸ crawl_footer_links: å­ãƒªãƒ³ã‚¯ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã«å¤±æ•—: {absolute_url} ({type(e).__name__})")
            markdown = ""

        max_length = 8000
        if len(markdown) > max_length:
            markdown = markdown[:max_length] + "\n\n[... trimmed ...]"

        links.append(
            {
                "url": absolute_url,
                "anchor_text": anchor_text,
                "content": markdown,
                "length": len(markdown),
            }
        )

    return {
        "success": True,
        "footer_text": footer_text,
        "links": links,
        "link_count": len(links),
    }


def handle_validate_company_info(args: Dict[str, Any]) -> Dict[str, Any]:
    """validate_company_infoé–¢æ•°ã®å‘¼ã³å‡ºã—ã‚’å‡¦ç†."""
    result = {
        "valid": True,
        "errors": [],
        "suggestions": {},
    }

    company = args.get("company", "").strip() if args.get("company") else ""
    tel = args.get("tel", "").strip() if args.get("tel") else ""
    address = args.get("address", "").strip() if args.get("address") else ""
    # business = args.get("business", "").strip() if args.get("business") else ""
    # genre = args.get("genre", "").strip() if args.get("genre") else ""
    # required_businesses = args.get("required_businesses", [])
    # required_genre = args.get("required_genre", [])

    # ä¼šç¤¾åã®æ¤œè¨¼
    if company:
        if not validate_company_format(company):
            result["valid"] = False
            result["errors"].append(
                "ä¼šç¤¾åã®å½¢å¼ãŒä¸æ­£ã§ã™ã€‚ã€Œæ ªå¼ä¼šç¤¾ã€ã€Œæœ‰é™ä¼šç¤¾ã€ã€ŒåˆåŒä¼šç¤¾ã€ãªã©ã‚’å«ã¿ã€"
                "æ”¯åº—ãƒ»å–¶æ¥­æ‰€ãƒ»æ‹¬å¼§ãƒ»ã‚¹ãƒšãƒ¼ã‚¹ã‚’å«ã¾ãšã€å…¨è§’è‹±æ•°å­—/è¨˜å·ã‚’å«ã¾ãªã„å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
            )
    else:
        result["errors"].append("ä¼šç¤¾åãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    # é›»è©±ç•ªå·ã®æ¤œè¨¼
    if tel:
        if not validate_tel_format(tel):
            result["valid"] = False
            result["errors"].append(
                "é›»è©±ç•ªå·ã®å½¢å¼ãŒä¸æ­£ã§ã™ã€‚åŠè§’æ•°å­—ã¨ãƒã‚¤ãƒ•ãƒ³ã®ã¿ã€ãƒã‚¤ãƒ•ãƒ³ã‚’å«ã‚€å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
            )
    else:
        result["errors"].append("é›»è©±ç•ªå·ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    # ä½æ‰€ã®æ¤œè¨¼
    if address:
        if not validate_address_format(address):
            result["valid"] = False
            result["errors"].append(
                "ä½æ‰€ã®å½¢å¼ãŒä¸æ­£ã§ã™ã€‚ã€Œéƒ½ã€ã€Œé“ã€ã€Œåºœã€ã€ŒçœŒã€ã®ã„ãšã‚Œã‹ã‚’å«ã‚€å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
            )
    else:
        result["errors"].append("ä½æ‰€ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    # æ¥­ç¨®ãƒ»ã‚¸ãƒ£ãƒ³ãƒ«ã®æ¤œè¨¼ã¯å‰Šé™¤ï¼ˆå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•è¨­å®šã•ã‚Œã‚‹ãŸã‚ï¼‰

    return result


def handle_function_call(function_name: str, args: Dict[str, Any]) -> Dict[str, Any]:
    """é–¢æ•°å‘¼ã³å‡ºã—ã‚’å‡¦ç†ã—ã¦çµæœè¾æ›¸ã‚’è¿”ã™."""
    if function_name == "check_url_accessibility":
        result = handle_check_url_accessibility(args)
    elif function_name == "crawl_website":
        result = handle_crawl_website(args)
    elif function_name == "crawl_footer_links":
        result = handle_crawl_footer_links(args)
    elif function_name == "validate_company_info":
        result = handle_validate_company_info(args)
    else:
        result = {"error": f"æœªçŸ¥ã®é–¢æ•°: {function_name}"}

    return result


def get_function_declarations() -> list[genai_types.FunctionDeclaration]:
    """å…¨ã¦ã®é–¢æ•°å®£è¨€ã‚’è¿”ã™."""
    return [
        get_check_url_accessibility_declaration(),
        get_crawl_website_declaration(),
        get_crawl_footer_links_declaration(),
        get_validate_company_info_declaration(),
    ]

