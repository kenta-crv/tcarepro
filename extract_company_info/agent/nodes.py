import re
import time

from google.genai.types import Tool as GenAITool, GoogleSearch
from google.api_core.exceptions import ResourceExhausted
from langchain_core.prompts.loading import load_prompt
from langchain_google_genai import ChatGoogleGenerativeAI

from agent.state import ExtractState
from models.schemas import CompanyInfo, LLMCompanyInfo, URLScoreList
from models.settings import BASE_DIR, settings
from utils.crawl4ai_util import crawl_markdown
from utils.net import convert_accessable_urls
from utils.logger import get_logger

RETRY_DELAY_SECONDS = 4.0
RETRY_ATTEMPTS = 1  # 1å›ãƒªãƒˆãƒ©ã‚¤ = æœ€å¤§2å›è©¦è¡Œ
API_CALL_INTERVAL_SECONDS = 2.0  # APIå‘¼ã³å‡ºã—é–“ã®é–“éš”ï¼ˆç§’ï¼‰

logger = get_logger()


# -----------------------------------------------------------------------------
# Company matching helpers
# -----------------------------------------------------------------------------
# NOTE:
# - We must avoid false positives where a different company is incorrectly accepted.
# - Therefore we only allow "corporate designator missing" relaxation when the
#   input company name uses prefix-style designator (e.g., "æ ªå¼ä¼šç¤¾â—¯â—¯", "åˆåŒä¼šç¤¾â—¯â—¯").
# - We additionally require location consistency (prefecture + if available city/ward).
PREFECTURES = [
    "åŒ—æµ·é“","é’æ£®çœŒ","å²©æ‰‹çœŒ","å®®åŸçœŒ","ç§‹ç”°çœŒ","å±±å½¢çœŒ","ç¦å³¶çœŒ",
    "èŒ¨åŸçœŒ","æ ƒæœ¨çœŒ","ç¾¤é¦¬çœŒ","åŸ¼ç‰çœŒ","åƒè‘‰çœŒ","æ±äº¬éƒ½","ç¥å¥ˆå·çœŒ",
    "æ–°æ½ŸçœŒ","å¯Œå±±çœŒ","çŸ³å·çœŒ","ç¦äº•çœŒ","å±±æ¢¨çœŒ","é•·é‡çœŒ",
    "å²é˜œçœŒ","é™å²¡çœŒ","æ„›çŸ¥çœŒ","ä¸‰é‡çœŒ",
    "æ»‹è³€çœŒ","äº¬éƒ½åºœ","å¤§é˜ªåºœ","å…µåº«çœŒ","å¥ˆè‰¯çœŒ","å’Œæ­Œå±±çœŒ",
    "é³¥å–çœŒ","å³¶æ ¹çœŒ","å²¡å±±çœŒ","åºƒå³¶çœŒ","å±±å£çœŒ",
    "å¾³å³¶çœŒ","é¦™å·çœŒ","æ„›åª›çœŒ","é«˜çŸ¥çœŒ",
    "ç¦å²¡çœŒ","ä½è³€çœŒ","é•·å´çœŒ","ç†Šæœ¬çœŒ","å¤§åˆ†çœŒ","å®®å´çœŒ","é¹¿å…å³¶çœŒ","æ²–ç¸„çœŒ",
]

LEGAL_DESIGNATORS_PREFIX = [
    # ä¼šç¤¾æ³•
    "æ ªå¼ä¼šç¤¾",
    "åˆåŒä¼šç¤¾",
    "åˆåä¼šç¤¾",
    "åˆè³‡ä¼šç¤¾",
    "ç‰¹ä¾‹æœ‰é™ä¼šç¤¾",
    "æœ‰é™ä¼šç¤¾",
    # ãã®ä»–ï¼ˆæ³•äººæ ¼ï¼‰
    "ç›¸äº’ä¼šç¤¾",
    "ä¸€èˆ¬ç¤¾å›£æ³•äºº",
    "ä¸€èˆ¬è²¡å›£æ³•äºº",
    "å…¬ç›Šç¤¾å›£æ³•äºº",
    "å…¬ç›Šè²¡å›£æ³•äºº",
    "NPOæ³•äºº",
    "ç‰¹å®šéå–¶åˆ©æ´»å‹•æ³•äºº",
    "åŒ»ç™‚æ³•äºº",
    "ç¤¾ä¼šåŒ»ç™‚æ³•äºº",
    "ç¤¾ä¼šç¦ç¥‰æ³•äºº",
    "å­¦æ ¡æ³•äºº",
    "å®—æ•™æ³•äºº",
    "æ›´ç”Ÿä¿è­·æ³•äºº",
    "åœ°æ–¹å…¬å…±å›£ä½“",
    "ç‹¬ç«‹è¡Œæ”¿æ³•äºº",
    "å›½ç«‹å¤§å­¦æ³•äºº",
    "å¼è­·å£«æ³•äºº",
    "ç¨ç†å£«æ³•äºº",
    "ç›£æŸ»æ³•äºº",
    "å¸æ³•æ›¸å£«æ³•äºº",
    "è¾²äº‹çµ„åˆæ³•äºº",
    "è¾²æ¥­å”åŒçµ„åˆ",
    "æ¶ˆè²»ç”Ÿæ´»å”åŒçµ„åˆ",
    "åŠ´åƒçµ„åˆ",
    "ç®¡ç†çµ„åˆæ³•äºº",
]

LEGAL_DESIGNATORS_SUFFIX = [
    # ã€Œå¾Œæ ªã€ç³»ï¼ˆã‚µã‚¤ãƒˆä¸Šã§é »å‡ºï¼‰
    "æ ªå¼ä¼šç¤¾",
    # å¿…è¦ãŒå‡ºãŸã‚‰ã“ã“ã«è¿½åŠ ï¼ˆä¾‹ï¼šæœ‰é™ä¼šç¤¾ ç­‰ï¼‰
]

_KABU_ABBREV_RE = re.compile(r"(?:ãˆ±|[ï¼ˆ(]\s*æ ª\s*[)ï¼‰])")

def _normalize_company_designators(name: str) -> str:
    """ä¼šç¤¾å½¢æ…‹ã®ç•¥è¨˜ã‚’æ­£è¦åŒ–ã™ã‚‹ï¼ˆè¡¨ç¤ºæºã‚Œå¸åç”¨ï¼‰.

    å¯¾è±¡:
      - ãˆ±, ï¼ˆæ ªï¼‰, (æ ª)  â†’ æ ªå¼ä¼šç¤¾
    """
    if not name:
        return name
    s = name.strip()
    s = _KABU_ABBREV_RE.sub("æ ªå¼ä¼šç¤¾", s)
    # é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
    s = re.sub(r"\s+", " ", s)
    return s


def _strip_spaces(s: str) -> str:
    return re.sub(r"\s+", "", s or "")


def _analyze_company_name(name: str) -> dict:

    def _normalize_corp_abbrev(s: str) -> str:
        # Common abbreviations: (æ ª),(æœ‰), ãˆ±,ãˆ², full-width parentheses too.
        if not s:
            return ""
        t = str(s)
        t = t.replace("ãˆ±", "æ ªå¼ä¼šç¤¾").replace("ãˆ²", "æœ‰é™ä¼šç¤¾")
        t = re.sub(r"[ï¼ˆ(]\s*æ ª\s*[)ï¼‰]", "æ ªå¼ä¼šç¤¾", t)
        t = re.sub(r"[ï¼ˆ(]\s*æœ‰\s*[)ï¼‰]", "æœ‰é™ä¼šç¤¾", t)
        return t

    """ä¼šç¤¾åã‚’ (æ³•äººç¨®åˆ¥, ä½ç½®, æœ¬ä½“åç§°) ã«åˆ†è§£ã™ã‚‹.

    è¿”å´ã‚­ãƒ¼ã¯æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯äº’æ›ã«ã—ã¦ã„ã‚‹:
      - raw: æ­£è¦åŒ–å¾Œã®åŸæ–‡
      - no_space: ç©ºç™½é™¤å»ç‰ˆ
      - designator: æ³•äººç¨®åˆ¥ï¼ˆæ ªå¼ä¼šç¤¾/åˆåŒä¼šç¤¾...ï¼‰ or None
      - position: 'prefix' / 'suffix' / None
      - core: æ³•äººç¨®åˆ¥ã‚’é™¤ã„ãŸåç§°ï¼ˆè‹±æ•°å­—ã¯å°æ–‡å­—åŒ–ï¼‰
      - has_designator: bool
    """
    if not name:
        return {
            "raw": "",
            "no_space": "",
            "designator": None,
            "position": None,
            "core": "",
            "has_designator": False,
        }

    normalized = _normalize_company_designators(name)
    raw = normalized.strip()
    no_space = _strip_spaces(raw)

    # prefix
    for d in LEGAL_DESIGNATORS_PREFIX:
        if raw.startswith(d):
            core = raw[len(d):].strip()
            return {
                "raw": raw,
                "no_space": no_space,
                "designator": d,
                "position": "prefix",
                "core": _strip_spaces(core).lower(),
                "has_designator": True,
            }

    # suffixï¼ˆå¾Œæ ªï¼‰
    for d in LEGAL_DESIGNATORS_SUFFIX:
        if raw.endswith(d) and len(raw) > len(d):
            core = raw[:-len(d)].strip()
            return {
                "raw": raw,
                "no_space": no_space,
                "designator": d,
                "position": "suffix",
                "core": _strip_spaces(core).lower(),
                "has_designator": True,
            }

    return {
        "raw": raw,
        "no_space": no_space,
        "designator": None,
        "position": None,
        "core": no_space.lower(),
        "has_designator": False,
    }


def _extract_pref_city(text: str) -> tuple[str, str]:
    """Extract (prefecture, municipality) from a Japanese address/location string.

    - prefecture: endswith éƒ½/é“/åºœ/çœŒ
    - municipality: last match of å¸‚/åŒº/ç”º/æ‘ after the prefecture (more specific wins)
      e.g. 'åŒ—æµ·é“ æœ­å¹Œå¸‚ æ‰‹ç¨²åŒº' -> ('åŒ—æµ·é“', 'æ‰‹ç¨²åŒº')
    """
    if not text:
        return ("", "")
    s = re.sub(r"\s+", "", str(text))

    # Prefecture: first occurrence
    m_pref = re.search(r"(.{1,3}?[éƒ½é“åºœçœŒ])", s)
    pref = m_pref.group(1) if m_pref else ""

    # Municipality candidates: å¸‚/åŒº/ç”º/æ‘ following the prefecture
    muni = ""
    if pref:
        after = s.split(pref, 1)[1]
    else:
        after = s

    # Capture sequences like 'æœ­å¹Œå¸‚', 'æ‰‹ç¨²åŒº', 'ä¸­åŸæ‘', etc.
    munis = re.findall(r"([^0-9\W]{1,20}?[å¸‚åŒºç”ºæ‘])", after)
    if munis:
        muni = munis[-1]  # choose the most specific (last)
    return (pref, muni)


def _location_consistent(input_location: str, extracted_address: str) -> bool:
    in_pref, in_city = _extract_pref_city(input_location)
    ex_pref, ex_city = _extract_pref_city(extracted_address)
    if in_pref and ex_pref and in_pref != ex_pref:
        return False
    # If user provided a city/ward, require it to match when we have it.
    if in_city:
        if not ex_city:
            # If extracted address doesn't include a city token, we can't be sure.
            return False
        if in_city != ex_city:
            return False
    return True

def _should_override_company_name(input_company: str, input_location: str, extracted_company: str, extracted_address: str) -> bool:
    """ä¼šç¤¾åã‚’ input.company ã§ä¸Šæ›¸ãã—ã¦ã‚ˆã„ã‚±ãƒ¼ã‚¹ã‹åˆ¤å®šã™ã‚‹.

    ç›®çš„:
      - LLMãŒã€Œæ ªå¼ä¼šç¤¾ã€ç­‰ã‚’è½ã¨ã™/ç•¥è¨˜ã™ã‚‹ã‚±ãƒ¼ã‚¹ã‚’æ•‘æ¸ˆã™ã‚‹ï¼ˆå‰æ ªãƒ»å¾Œæ ªã©ã¡ã‚‰ã‚‚ï¼‰
      - ãŸã ã—ã€å‰æ ªâ†”å¾Œæ ªã®å–ã‚Šé•ãˆï¼ˆä¾‹: 'æ ªå¼ä¼šç¤¾A' vs 'Aæ ªå¼ä¼šç¤¾'ï¼‰ã¯æ•‘æ¸ˆã—ãªã„
        â€» ãã®å ´åˆã¯ä¸Šæµã§ä¸ä¸€è‡´ã¨ã—ã¦å¼¾ãã¹ã

    å…·ä½“ä¾‹ï¼ˆOKï¼‰:
      - æ ªå¼ä¼šç¤¾ã‚µã‚«ã‚¤å¼•è¶Šã‚»ãƒ³ã‚¿ãƒ¼ â‡” ã‚µã‚«ã‚¤å¼•è¶Šã‚»ãƒ³ã‚¿ãƒ¼
      - ã‚µã‚«ã‚¤å¼•è¶Šã‚»ãƒ³ã‚¿ãƒ¼æ ªå¼ä¼šç¤¾ â‡” ã‚µã‚«ã‚¤å¼•è¶Šã‚»ãƒ³ã‚¿ãƒ¼
      - ï¼ˆæ ªï¼‰ã‚µã‚«ã‚¤å¼•è¶Šã‚»ãƒ³ã‚¿ãƒ¼ â‡” ã‚µã‚«ã‚¤å¼•è¶Šã‚»ãƒ³ã‚¿ãƒ¼
      - ã‚µã‚«ã‚¤å¼•è¶Šã‚»ãƒ³ã‚¿ãƒ¼(æ ª) â‡” ã‚µã‚«ã‚¤å¼•è¶Šã‚»ãƒ³ã‚¿ãƒ¼

    å…·ä½“ä¾‹ï¼ˆNGï¼‰:
      - æ ªå¼ä¼šç¤¾ã‚µã‚«ã‚¤å¼•è¶Šã‚»ãƒ³ã‚¿ãƒ¼ â‡” ã‚µã‚«ã‚¤å¼•è¶Šã‚»ãƒ³ã‚¿ãƒ¼æ ªå¼ä¼šç¤¾ï¼ˆå‰æ ªâ†”å¾Œæ ªï¼‰
    """
    in_a = _analyze_company_name(input_company)
    ex_a = _analyze_company_name(extracted_company)

    # æœ¬ä½“ãŒä¸€è‡´ã—ãªã„ãªã‚‰ä¸å¯
    if in_a["core"] != ex_a["core"]:
        return False

    # æ‰€åœ¨åœ°ãŒæ•´åˆã—ãªã„ãªã‚‰ä¸å¯
    if not _location_consistent(input_location, extracted_address):
        return False

    # ä¼šç¤¾å½¢æ…‹ã®æ‰±ã„:
    # - input ãŒæ³•äººç¨®åˆ¥ã‚’æŒã¡ã€extracted ãŒæŒãŸãªã„ï¼ˆè½ã¡/ç•¥è¨˜ï¼‰ã®ã¿ä¸Šæ›¸ãã‚’è¨±å¯
    #   ï¼ˆå‰æ ªãƒ»å¾Œæ ªã©ã¡ã‚‰ã® input ã§ã‚‚è¨±å¯ã™ã‚‹ï¼‰
    if in_a["has_designator"] and not ex_a["has_designator"]:
        return True

    # extracted ãŒæ³•äººç¨®åˆ¥ã‚’æŒã¤å ´åˆã¯ã€ã“ã“ã§ã¯ä¸Šæ›¸ãã—ãªã„
    # ï¼ˆä¾‹: input ãŒç•¥ç§°/å±‹å·ã§ extracted ãŒæ­£å¼åç§°ã®å ´åˆã‚‚ã‚ã‚‹ï¼‰
    return False

def _invoke_with_retry(llm, prompt_str: str, *, retries: int = RETRY_ATTEMPTS, **invoke_kwargs):
    """Gemini APIå‘¼ã³å‡ºã—ã‚’æœ€å¤§retrieså›å†è©¦è¡Œï¼ˆ4ç§’å¾…æ©Ÿï¼‰ã§å®Ÿè¡Œ."""
    attempts = retries + 1
    for attempt in range(attempts):
        try:
            return llm.invoke(prompt_str, **invoke_kwargs)
        except ResourceExhausted as exc:
            if attempt == retries:
                raise
            logger.warning(
                "  âš ï¸ Gemini APIã‚¯ã‚©ãƒ¼ã‚¿è¶…é (attempt %s/%s). %sç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™â€¦",
                attempt + 1,
                attempts,
                RETRY_DELAY_SECONDS,
            )
            time.sleep(RETRY_DELAY_SECONDS)
        except Exception:
            raise


def _wait_between_api_calls():
    """APIå‘¼ã³å‡ºã—é–“ã®é–“éš”ã‚’ç©ºã‘ã‚‹."""
    logger.debug(f"  â³ APIå‘¼ã³å‡ºã—é–“éš”ã®ãŸã‚{API_CALL_INTERVAL_SECONDS}ç§’å¾…æ©Ÿä¸­...")
    time.sleep(API_CALL_INTERVAL_SECONDS)


def node_get_url_candidates(state: ExtractState) -> ExtractState:
    """ä¼šç¤¾åãƒ»å‹¤å‹™åœ°ã‹ã‚‰URLå€™è£œã‚’æ¤œç´¢ã—ã¦çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹.

    Args:
        state: LangGraphã®çŠ¶æ…‹ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªï¼ˆ`company`, `location` ã‚’æƒ³å®šï¼‰ã€‚

    Returns:
        dict: `urls` ã‚­ãƒ¼ã«å€™è£œURLã®é…åˆ—ã‚’è¿½åŠ ã—ãŸæ–°ã—ã„çŠ¶æ…‹ã€‚

    """
    node_start = time.time()
    logger.info("-" * 60)
    logger.info("[NODE 1/3] node_get_url_candidates - URLå€™è£œã®å–å¾—")
    logger.info(f"  å…¥åŠ›: {state.company} @ {state.location}")
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’YAMLã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
    prompt = load_prompt(str(BASE_DIR / "agent/prompts/extract_url.yaml"), encoding="utf-8")
    logger.debug("  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ãƒ¼ãƒ‰å®Œäº†")

    # LLMï¼ˆæ¤œç´¢ãƒ„ãƒ¼ãƒ«æœ‰åŠ¹ï¼‰ã‚’å‘¼ã³å‡ºã—
    # max_retries=2ã«åˆ¶é™ã—ã¦ç„¡é™ãƒ«ãƒ¼ãƒ—ã‚’é˜²ã
    logger.info("  ğŸ¤– Gemini APIå‘¼ã³å‡ºã—ä¸­ï¼ˆGoogleæ¤œç´¢ãƒ„ãƒ¼ãƒ«æœ‰åŠ¹ï¼‰...")
    api_start = time.time()
    
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0,
            google_api_key=settings.GOOGLE_API_KEY,
            max_retries=0,
        )
        # â˜…è¿½åŠ ï¼šãƒªãƒˆãƒ©ã‚¤ç”¨ã®æ¤œç´¢è£œåŠ©èªï¼ˆstate.search_hintãŒã‚ã‚Œã°ä½¿ã†ï¼‰
        hint = (getattr(state, "search_hint", None) or "").strip()
        
        company_q = state.company
        location_q = state.location
        
        # â˜…æ¤œç´¢ç”¨ã¯åˆ¥å¤‰æ•°ã«ã™ã‚‹ï¼ˆlocation_q / state.location ã¯ãã®ã¾ã¾ï¼‰
        search_location = location_q
        if hint:
            search_location = f"{location_q} {hint}".strip()
            logger.info("  ğŸ” ãƒªãƒˆãƒ©ã‚¤ç”¨æ¤œç´¢è£œåŠ©èªã‚’ä»˜ä¸: %s", hint)
        
        logger.info("  ğŸ” æ¤œç´¢å…¥åŠ›: %s @ %s", company_q, search_location)
        
        resp = _invoke_with_retry(
            llm,
            prompt.format(company=company_q, location=search_location),
            tools=[GenAITool(google_search=GoogleSearch())],
        )

        api_elapsed = time.time() - api_start
        logger.info(f"  âœ… APIå‘¼ã³å‡ºã—æˆåŠŸ ({api_elapsed:.2f}ç§’)")
        _wait_between_api_calls()  # APIå‘¼ã³å‡ºã—é–“ã®é–“éš”
    except Exception as e:
        api_elapsed = time.time() - api_start
        logger.error(f"  âŒ APIå‘¼ã³å‡ºã—å¤±æ•— ({api_elapsed:.2f}ç§’)")
        logger.error(f"  ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
        raise

    # å¿œç­”ã‹ã‚‰URLã‚’æŠ½å‡º
    urls: list[str] = []
    
    # ã¾ãšæœ¬æ–‡ã‹ã‚‰å…¨ã¦ã®URLã‚’æŠ½å‡ºï¼ˆæœ€ã‚‚ä¿¡é ¼æ€§ãŒé«˜ã„ï¼‰
    # ã‚ˆã‚Šå³å¯†ãªURLæ­£è¦è¡¨ç¾ã‚’ä½¿ç”¨ï¼ˆä¸å®Œå…¨ãªURLã‚’é™¤å¤–ï¼‰
    url_pattern = r'https?://[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*(?:/[^\s<>"]*)?'
    #content_urls = re.findall(url_pattern, resp.content)
    # resp.contentãŒãƒªã‚¹ãƒˆã®å ´åˆã¯æ–‡å­—åˆ—ã«å¤‰æ›
    content = resp.content if isinstance(resp.content, str) else str(resp.content)
    content_urls = re.findall(url_pattern, content)
    # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã¨ä¸å®Œå…¨ãªURLã‚’é™¤å¤–
    def _is_valid_url(url: str) -> bool:
        """URLãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹."""
        # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‚’é™¤å¤–
        if 'grounding-api-redirect' in url:
            return False
        # ã‚¹ã‚­ãƒ¼ãƒ ã¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å«ã‚€å¿…è¦ãŒã‚ã‚‹
        if url.count('/') < 2:
            return False
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ãƒ‰ãƒƒãƒˆã‚’å«ã‚€å¿…è¦ãŒã‚ã‚‹
        try:
            domain = url.split('//')[1].split('/')[0]
            if '.' not in domain:
                return False
            # æ—¥æœ¬èªæ–‡å­—ã‚„å…¨è§’æ–‡å­—ã‚’å«ã‚€URLã‚’é™¤å¤–ï¼ˆä¸å®Œå…¨ãªæŠ½å‡ºã‚’é˜²ãï¼‰
            if any(ord(c) > 127 for c in url):
                return False
        except (IndexError, AttributeError):
            return False
        return True
    
    content_urls = [url for url in content_urls if _is_valid_url(url)]
    
    if content_urls:
        urls.extend(content_urls)
        logger.info(f"  âœ… æœ¬æ–‡ã‹ã‚‰{len(content_urls)}å€‹ã®URLæŠ½å‡º:")
        for url in content_urls[:5]:
            logger.info(f"     - {url}")
    
    # groundingç”±æ¥URLï¼ˆãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã¯ä½¿ã‚ãªã„ï¼‰
    try:
        reference_urls = [
            chunk["web"]["uri"]
            for chunk in resp.response_metadata["grounding_metadata"]["grounding_chunks"]
        ]
        # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‚’é™¤å¤–
        #direct_urls = [url for url in reference_urls if not url.startswith('https://vertexaisearch.cloud.google.com')]
        # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‹ã‚‰å®Ÿéš›ã®URLã‚’å–å¾—
        import requests
        direct_urls = []
        for url in reference_urls:
            if url.startswith('https://vertexaisearch.cloud.google.com'):
                try:
                    r = requests.head(url, allow_redirects=True, timeout=5)
                    if r.url and not r.url.startswith('https://vertexaisearch'):
                        direct_urls.append(r.url)
                except:
                    pass
            else:
                direct_urls.append(url)
        if direct_urls:
            logger.info(f"  âœ… Googleæ¤œç´¢ã‹ã‚‰{len(direct_urls)}å€‹ã®ç›´æ¥URLå–å¾—")
            urls.extend(direct_urls)
        else:
            logger.warning(f"  âš ï¸ Googleæ¤œç´¢çµæœã¯å…¨ã¦ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLï¼ˆ{len(reference_urls)}å€‹ï¼‰- ã‚¹ã‚­ãƒƒãƒ—")
            for url in reference_urls:
                logger.warning(f"    ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURL: {url}")
    except Exception:  # noqa: BLE001
        logger.warning("  âš ï¸ Googleæ¤œç´¢çµæœãªã—")
    
    logger.info(f"  å–å¾—ã—ãŸURLå€™è£œ: {len(urls)}å€‹")

    # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³è¨­å®šã«åŸºã¥ã„ã¦å€™è£œURLã‚’ãƒ•ã‚£ãƒ«ã‚¿
    # ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚‚å«ã‚ã¦æœ«å°¾ä¸€è‡´ã§é™¤å¤–ã™ã‚‹
    def _is_excluded(url: str) -> bool:
        # EXCLUDE_DOMAINS ã¯æ”¹è¡ŒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ— or ãƒªã‚¹ãƒˆã‚’æƒ³å®š
        raw = settings.EXCLUDE_DOMAINS
        domains = [s.strip() for s in raw.splitlines() if s.strip()]
        return any(domain in url for domain in domains)

    filtered_urls = [u for u in urls if not _is_excluded(u)]
    excluded_count = len(urls) - len(filtered_urls)
    if excluded_count > 0:
        logger.info(f"  é™¤å¤–ã•ã‚ŒãŸURL: {excluded_count}å€‹")

    # åˆ°é”å¯èƒ½URLã«æ­£è¦åŒ–
    logger.info("  ğŸŒ URLåˆ°é”å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯ä¸­...")
    state.urls = convert_accessable_urls(filtered_urls)
    
    node_elapsed = time.time() - node_start
    logger.info(f"  âœ… æœ€çµ‚URLå€™è£œ: {len(state.urls)}å€‹")
    for i, url in enumerate(state.urls[:5], 1):  # æœ€å¤§5å€‹ã¾ã§è¡¨ç¤º
        logger.info(f"     {i}. {url}")
    if len(state.urls) > 5:
        logger.info(f"     ... ä»–{len(state.urls) - 5}å€‹")
    logger.info(f"  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: {node_elapsed:.2f}ç§’")
    
    return state


def node_select_official_website(state: ExtractState) -> ExtractState:
    """å€™è£œURLã‹ã‚‰å…¬å¼ã‚µã‚¤ãƒˆã‚’ä¸€ã¤é¸å®šã—ã¦çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹.

    Args:
        state: `urls` ã‚’å«ã‚€çŠ¶æ…‹ã€‚

    Returns:
        dict: `selected_url` ã‚’è¿½åŠ ã—ãŸçŠ¶æ…‹ã€‚

    """
    node_start = time.time()
    logger.info("-" * 60)
    logger.info("[NODE 2/3] node_select_official_website - å…¬å¼ã‚µã‚¤ãƒˆé¸å®š")
    logger.info(f"  å€™è£œURLæ•°: {len(state.urls)}å€‹")
    
    # URLå€™è£œãŒ1å€‹ä»¥ä¸‹ã®å ´åˆã€é¸å®šä¸è¦ï¼ˆæœ€é©åŒ–ï¼‰
    if len(state.urls) <= 1:
        logger.info("  â„¹ï¸ URLå€™è£œãŒ1å€‹ä»¥ä¸‹ã®ãŸã‚é¸å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        logger.info("  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: 0.00ç§’ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        return state
    
    urls = state.urls
    web_context = ""
    
    logger.info("  ğŸ•·ï¸ å„URLã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­ï¼ˆtimeout=20ç§’ï¼‰...")
    for i, url in enumerate(urls, 1):
        crawl_start = time.time()
        logger.info(f"     [{i}/{len(urls)}] {url}")
        markdown = crawl_markdown(url, timeout=20)
        crawl_elapsed = time.time() - crawl_start
        if not markdown:
            logger.warning(f"        âš ï¸ ã‚¯ãƒ­ãƒ¼ãƒ«å¤±æ•—ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({crawl_elapsed:.2f}ç§’)")
            continue  # å¤±æ•—ã—ãŸURLã¯ã‚¹ã‚­ãƒƒãƒ—
        logger.info(f"        âœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº† ({crawl_elapsed:.2f}ç§’, {len(markdown)}æ–‡å­—)")
        web_context += f"""# {url}\n{markdown}\n"""
    
    prompt = load_prompt(str(BASE_DIR / "agent/prompts/select_official.yaml"), encoding="utf-8")
    logger.debug("  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ãƒ¼ãƒ‰å®Œäº†")
    
    logger.info("  ğŸ¤– Gemini APIå‘¼ã³å‡ºã—ä¸­ï¼ˆå…¬å¼ã‚µã‚¤ãƒˆé¸å®šï¼‰...")
    api_start = time.time()
    
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0,
            google_api_key=settings.GOOGLE_API_KEY,
            max_retries=0,
        ).with_structured_output(URLScoreList)
        resp: URLScoreList = _invoke_with_retry(
            llm,
            prompt.format(company=state.company, location=state.location, web_context=web_context),
        )
        api_elapsed = time.time() - api_start
        logger.info(f"  âœ… APIå‘¼ã³å‡ºã—æˆåŠŸ ({api_elapsed:.2f}ç§’)")
        _wait_between_api_calls()  # APIå‘¼ã³å‡ºã—é–“ã®é–“éš”
    except Exception as e:
        api_elapsed = time.time() - api_start
        logger.error(f"  âŒ APIå‘¼ã³å‡ºã—å¤±æ•— ({api_elapsed:.2f}ç§’)")
        logger.error(f"  ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
        raise
    
    sorted_urls = sorted(resp.urls, key=lambda x: x.score, reverse=True)
    logger.info("  ğŸ“Š URLã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°çµæœ:")
    for i, url_score in enumerate(sorted_urls[:5], 1):
        logger.info(f"     {i}. {url_score.url} (ã‚¹ã‚³ã‚¢: {url_score.score})")

    def _is_excluded(url: str) -> bool:
        # EXCLUDE_DOMAINS ã¯æ”¹è¡ŒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ— or ãƒªã‚¹ãƒˆã‚’æƒ³å®š
        raw = settings.EXCLUDE_DOMAINS
        domains = [s.strip() for s in raw.splitlines() if s.strip()]
        return any(domain in url for domain in domains)

    state.urls = [url.url for url in sorted_urls if not _is_excluded(url.url)]
    
    node_elapsed = time.time() - node_start
    logger.info(f"  âœ… é¸å®šã•ã‚ŒãŸURL: {len(state.urls)}å€‹")
    logger.info(f"  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: {node_elapsed:.2f}ç§’")

    return state


def node_fetch_html(state: ExtractState) -> ExtractState:
    """é¸å®šURLã®HTMLã‚’å–å¾—ã—çŠ¶æ…‹ã«æ ¼ç´ã™ã‚‹.

    Args:
        state: `selected_url` ã‚’å«ã‚€çŠ¶æ…‹ã€‚

    Returns:
        dict: `html` ã‚’è¿½åŠ ã—ãŸçŠ¶æ…‹ï¼ˆå¤±æ•—æ™‚ã¯ç©ºæ–‡å­—ï¼‰ã€‚

    """
    node_start = time.time()
    logger.info("-" * 60)
    logger.info("[NODE 3/3] node_fetch_html - ä¼šç¤¾æƒ…å ±æŠ½å‡º")
    
    # URLå€™è£œãŒç„¡ã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹ï¼ˆValidationErrorã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
    if not state.urls:
        logger.warning("  âš ï¸ URLå€™è£œãŒ0å€‹ - æŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        raise ValueError("URLå€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ä¼šç¤¾æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã€‚")
    
    url = state.urls.pop(0)
    logger.info(f"  å¯¾è±¡URL: {url}")
    
    logger.info("  ğŸ•·ï¸ Webãƒšãƒ¼ã‚¸ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­ï¼ˆdepth=1, timeout=60ç§’ï¼‰...")
    crawl_start = time.time()
    web_context = crawl_markdown(url, depth=1, timeout=60)
    crawl_elapsed = time.time() - crawl_start
    if not web_context:
        logger.warning(f"  âš ï¸ ã‚¯ãƒ­ãƒ¼ãƒ«å¤±æ•—ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({crawl_elapsed:.2f}ç§’)")
        raise ValueError(f"URL {url} ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ï¼‰ã€‚")
    logger.info(f"  âœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº† ({crawl_elapsed:.2f}ç§’, {len(web_context)}æ–‡å­—)")

    prompt = load_prompt(str(BASE_DIR / "agent/prompts/extract_contact.yaml"), encoding="utf-8")
    logger.debug("  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ãƒ¼ãƒ‰å®Œäº†")
    
    logger.info("  ğŸ¤– Gemini APIå‘¼ã³å‡ºã—ä¸­ï¼ˆä¼šç¤¾æƒ…å ±æŠ½å‡ºï¼‰...")
    logger.info(f"     å¿…é ˆæ¥­ç¨®: {state.required_businesses}")
    logger.info(f"     å¿…é ˆã‚¸ãƒ£ãƒ³ãƒ«: {state.required_genre}")
    api_start = time.time()
    
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0,
            google_api_key=settings.GOOGLE_API_KEY,
            max_retries=0,
        ).with_structured_output(LLMCompanyInfo)
        resp: LLMCompanyInfo = _invoke_with_retry(
            llm,
            prompt.format(
                required_businesses=state.required_businesses,
                required_genre=state.required_genre,
                web_context=web_context,
            ),
        )
        api_elapsed = time.time() - api_start
        logger.info(f"  âœ… APIå‘¼ã³å‡ºã—æˆåŠŸ ({api_elapsed:.2f}ç§’)")
        # æœ€å¾Œã®APIå‘¼ã³å‡ºã—ãªã®ã§é–“éš”ã¯ä¸è¦
        logger.info("  ğŸ“‹ æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±:")
        logger.info(f"     ä¼šç¤¾å: {resp.company}")
        logger.info(f"     é›»è©±ç•ªå·: {resp.tel}")
        logger.info(f"     ä½æ‰€: {resp.address}")
        logger.info(f"     URL: {resp.url}")
        logger.info(f"     ãŠå•ã„åˆã‚ã›URL: {resp.contact_url}")
    except Exception as e:
        api_elapsed = time.time() - api_start
        logger.error(f"  âŒ APIå‘¼ã³å‡ºã—å¤±æ•— ({api_elapsed:.2f}ç§’)")
        logger.error(f"  ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
        raise

    #state.company_info = resp.model_dump()
    # ä½æ‰€ãŒç©ºã®å ´åˆã€å…¥åŠ›ã®locationã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨
    if not resp.address or resp.address.strip() == "":
        logger.warning(f"  âš ï¸ ä½æ‰€ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å…¥åŠ›ã®locationã‚’ä½¿ç”¨: {state.location}")
        resp.address = state.location.replace(" ", "")  # ã‚¹ãƒšãƒ¼ã‚¹ã‚’é™¤å»


    # ä¼šç¤¾åã®æºã‚Œè£œæ­£ï¼ˆå®‰å…¨å´ï¼‰
    # - LLMãŒã€Œæ ªå¼ä¼šç¤¾ã€ç­‰ã‚’çœç•¥ã—ãŸã‚Šã€è‹±å­—ã®å¤§å°ã®ã¿ãŒæºã‚Œã‚‹ã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ãŸã‚ã€
    #   ä½æ‰€ï¼ˆéƒ½é“åºœçœŒãƒ»å¯èƒ½ãªã‚‰å¸‚åŒºç”ºæ‘ï¼‰ã¨ä¼šç¤¾åã‚³ã‚¢ãŒä¸€è‡´ã™ã‚‹å ´åˆã®ã¿ input.company ã§è£œå®Œã™ã‚‹ã€‚
    if resp.company and state.company and resp.address:
        if _should_override_company_name(state.company, state.location, resp.company, resp.address):
            if resp.company != state.company:
                logger.warning(
                    f"  âš ï¸ ä¼šç¤¾åã®è¡¨è¨˜æºã‚Œã‚’è£œæ­£ã—ã¾ã™: extracted='{resp.company}' -> input='{state.company}'"
                )
            resp.company = state.company

    state.company_info = resp.model_dump()
    
    node_elapsed = time.time() - node_start
    logger.info(f"  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: {node_elapsed:.2f}ç§’")
    
    return state
