import re
import time

from google.ai.generativelanguage_v1beta.types import Tool as GenAITool
from google.api_core.exceptions import ResourceExhausted
from langchain_core.prompts.loading import load_prompt
from langchain_google_genai import ChatGoogleGenerativeAI

from agent.state import ExtractState
from models.schemas import CompanyInfo, LLMCompanyInfo, URLScoreList
from models.settings import BASE_DIR, settings
from utils.crawl4ai_util import crawl_markdown
from utils.net import convert_accessable_urls
from utils.logger import get_logger

RETRY_DELAY_SECONDS = 8.0  # ãƒªãƒˆãƒ©ã‚¤æ™‚ã®å¾…æ©Ÿæ™‚é–“ã‚’8ç§’ã«å¢—åŠ 
RETRY_ATTEMPTS = 1  # 1å›ãƒªãƒˆãƒ©ã‚¤ = æœ€å¤§2å›è©¦è¡Œ
API_CALL_INTERVAL_SECONDS = 5.0  # APIå‘¼ã³å‡ºã—é–“ã®é–“éš”ã‚’5ç§’ã«å¢—åŠ ï¼ˆRPM=15ã®å ´åˆã€æœ€ä½4ç§’å¿…è¦ï¼‰

logger = get_logger()


def _invoke_with_retry(llm, prompt_str: str, *, retries: int = RETRY_ATTEMPTS, **invoke_kwargs):
    """Gemini APIå‘¼ã³å‡ºã—ã‚’æœ€å¤§retrieså›å†è©¦è¡Œï¼ˆã‚¨ã‚¯ã‚¹ãƒãƒãƒ³ã‚·ãƒ£ãƒ«ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰ã§å®Ÿè¡Œ."""
    attempts = retries + 1
    for attempt in range(attempts):
        try:
            return llm.invoke(prompt_str, **invoke_kwargs)
        except ResourceExhausted as exc:
            if attempt == retries:
                # æœ€å¾Œã®è©¦è¡Œã§ã‚‚å¤±æ•—ã—ãŸå ´åˆã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è©³ç´°ã«ãƒ­ã‚°å‡ºåŠ›
                error_msg = str(exc)
                logger.error(f"  âŒ ResourceExhaustedã‚¨ãƒ©ãƒ¼ï¼ˆæœ€çµ‚è©¦è¡Œå¤±æ•—ï¼‰: {error_msg[:300]}")
                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ã‚¯ã‚©ãƒ¼ã‚¿é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                if "quota" in error_msg.lower() or "limit" in error_msg.lower():
                    logger.error("  âš ï¸ ã‚¯ã‚©ãƒ¼ã‚¿/åˆ¶é™é–¢é€£ã®ã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                else:
                    logger.warning("  âš ï¸ ä¸€æ™‚çš„ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ˆã‚¯ã‚©ãƒ¼ã‚¿è¶…éã§ã¯ãªã„å¯èƒ½æ€§ï¼‰")
                raise
            # ã‚¨ã‚¯ã‚¹ãƒãƒãƒ³ã‚·ãƒ£ãƒ«ãƒãƒƒã‚¯ã‚ªãƒ•: 2^attempt * RETRY_DELAY_SECONDS
            backoff_delay = RETRY_DELAY_SECONDS * (2 ** attempt)
            logger.warning(
                "  âš ï¸ ResourceExhaustedã‚¨ãƒ©ãƒ¼ (attempt %s/%s). %sç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™â€¦",
                attempt + 1,
                attempts,
                backoff_delay,
            )
            logger.debug(f"  ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(exc)[:200]}")
            time.sleep(backoff_delay)
        except Exception:
            raise


def _wait_between_api_calls():
    """APIå‘¼ã³å‡ºã—é–“ã®é–“éš”ã‚’ç©ºã‘ã‚‹."""
    logger.debug(f"  â³ APIå‘¼ã³å‡ºã—é–“éš”ã®ãŸã‚{API_CALL_INTERVAL_SECONDS}ç§’å¾…æ©Ÿä¸­...")
    time.sleep(API_CALL_INTERVAL_SECONDS)


def node_get_url_candidates(state: ExtractState) -> ExtractState:
    """ä¼šç¤¾åãƒ»å‹¤å‹™åœ°ã‹ã‚‰URLå€™è£œã‚’æ¤œç´¢ã—ã¦çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹.

    Args:
        state: LangGraphã®çŠ¶æ…‹ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªï¼ˆ`company`, `location` ã‚’æƒ³å®šï¼‰ã€‚

    Returns:
        dict: `urls` ã‚­ãƒ¼ã«å€™è£œURLã®é…åˆ—ã‚’è¿½åŠ ã—ãŸæ–°ã—ã„çŠ¶æ…‹ã€‚

    """
    node_start = time.time()
    logger.info("-" * 60)
    logger.info("[NODE 1/3] node_get_url_candidates - URLå€™è£œã®å–å¾—")
    logger.info(f"  å…¥åŠ›: {state.company} @ {state.location}")
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’YAMLã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
    prompt = load_prompt(str(BASE_DIR / "agent/prompts/extract_url.yaml"), encoding="utf-8")
    logger.debug("  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ãƒ¼ãƒ‰å®Œäº†")

    # LLMï¼ˆæ¤œç´¢ãƒ„ãƒ¼ãƒ«æœ‰åŠ¹ï¼‰ã‚’å‘¼ã³å‡ºã—
    # max_retries=2ã«åˆ¶é™ã—ã¦ç„¡é™ãƒ«ãƒ¼ãƒ—ã‚’é˜²ã
    logger.info("  ğŸ¤– Gemini APIå‘¼ã³å‡ºã—ä¸­ï¼ˆGoogleæ¤œç´¢ãƒ„ãƒ¼ãƒ«æœ‰åŠ¹ï¼‰...")
    logger.info("  ğŸ” Google Searchãƒ„ãƒ¼ãƒ«ï¼ˆGrounding APIï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™")
    api_start = time.time()
    
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash",
            temperature=0,
            google_api_key=settings.GOOGLE_API_KEY,
            max_retries=0,
        )
        # Google Searchãƒ„ãƒ¼ãƒ«ä½¿ç”¨æ™‚ã¯è¿½åŠ ã®å¾…æ©Ÿæ™‚é–“ã‚’è¨­å®šï¼ˆGrounding APIã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ï¼‰
        google_search_tool = GenAITool(google_search={})
        resp = _invoke_with_retry(
            llm,
            prompt.format(company=state.company, location=state.location),
            tools=[google_search_tool],
        )
        api_elapsed = time.time() - api_start
        
        # å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        actual_model = "ä¸æ˜"
        try:
            if hasattr(resp, 'response_metadata') and resp.response_metadata:
                # response_metadataã‹ã‚‰ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—
                metadata = resp.response_metadata
                if isinstance(metadata, dict):
                    actual_model = metadata.get('model_name', metadata.get('model', 'ä¸æ˜'))
                elif hasattr(metadata, 'model_name'):
                    actual_model = metadata.model_name
                elif hasattr(metadata, 'model'):
                    actual_model = metadata.model
        except Exception:
            pass
        
        logger.info(f"  âœ… APIå‘¼ã³å‡ºã—æˆåŠŸ ({api_elapsed:.2f}ç§’)")
        specified_model = getattr(llm, 'model', getattr(llm, 'model_name', 'gemini-2.0-flash'))
        logger.info(f"  ğŸ“Š ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: æŒ‡å®š={specified_model}, å®Ÿéš›={actual_model}")
        
        # Google Searchãƒ„ãƒ¼ãƒ«ä½¿ç”¨æ™‚ã®å‡¦ç†
        # å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: https://ai.google.dev/gemini-api/docs/google-search?hl=ja
        # Google Searchãƒ„ãƒ¼ãƒ«ã¯å†…éƒ¨çš„ã«è¤‡æ•°ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€
        # é€šå¸¸ã®APIå‘¼ã³å‡ºã—ã‚ˆã‚Šã‚‚é•·ã„å¾…æ©Ÿæ™‚é–“ã‚’è¨­å®š
        # ãŸã ã—ã€å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¯å›ºæœ‰ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®è¨˜è¼‰ã¯ãªã„
        logger.debug("  â³ Google Searchãƒ„ãƒ¼ãƒ«ä½¿ç”¨å¾Œã®è¿½åŠ å¾…æ©Ÿæ™‚é–“ï¼ˆ2ç§’ï¼‰...")
        time.sleep(2.0)
        _wait_between_api_calls()  # APIå‘¼ã³å‡ºã—é–“ã®é–“éš”ï¼ˆé€šå¸¸ã®5ç§’ï¼‰
    except Exception as e:
        api_elapsed = time.time() - api_start
        logger.error(f"  âŒ APIå‘¼ã³å‡ºã—å¤±æ•— ({api_elapsed:.2f}ç§’)")
        logger.error(f"  ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
        raise

    # å¿œç­”ã‹ã‚‰URLã‚’æŠ½å‡º
    urls: list[str] = []
    
    # ã¾ãšæœ¬æ–‡ã‹ã‚‰å…¨ã¦ã®URLã‚’æŠ½å‡ºï¼ˆæœ€ã‚‚ä¿¡é ¼æ€§ãŒé«˜ã„ï¼‰
    # ã‚ˆã‚Šå³å¯†ãªURLæ­£è¦è¡¨ç¾ã‚’ä½¿ç”¨ï¼ˆä¸å®Œå…¨ãªURLã‚’é™¤å¤–ï¼‰
    url_pattern = r'https?://[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*(?:/[^\s<>"]*)?'
    content_urls = re.findall(url_pattern, resp.content)
    # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã¨ä¸å®Œå…¨ãªURLã‚’é™¤å¤–
    def _is_valid_url(url: str) -> bool:
        """URLãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹."""
        # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‚’é™¤å¤–
        if 'grounding-api-redirect' in url:
            return False
        # ã‚¹ã‚­ãƒ¼ãƒ ã¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å«ã‚€å¿…è¦ãŒã‚ã‚‹
        if url.count('/') < 2:
            return False
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ãƒ‰ãƒƒãƒˆã‚’å«ã‚€å¿…è¦ãŒã‚ã‚‹
        try:
            domain = url.split('//')[1].split('/')[0]
            if '.' not in domain:
                return False
            # æ—¥æœ¬èªæ–‡å­—ã‚„å…¨è§’æ–‡å­—ã‚’å«ã‚€URLã‚’é™¤å¤–ï¼ˆä¸å®Œå…¨ãªæŠ½å‡ºã‚’é˜²ãï¼‰
            if any(ord(c) > 127 for c in url):
                return False
        except (IndexError, AttributeError):
            return False
        return True
    
    content_urls = [url for url in content_urls if _is_valid_url(url)]
    
    if content_urls:
        urls.extend(content_urls)
        logger.info(f"  âœ… æœ¬æ–‡ã‹ã‚‰{len(content_urls)}å€‹ã®URLæŠ½å‡º:")
        for url in content_urls[:5]:
            logger.info(f"     - {url}")
    
    # groundingç”±æ¥URLï¼ˆãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‹ã‚‰å®Ÿéš›ã®URLã‚’æŠ½å‡ºï¼‰
    try:
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°ã«å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        logger.debug("  ğŸ“‹ response_metadataæ§‹é€ :")
        logger.debug(f"    response_metadata keys: {list(resp.response_metadata.keys()) if isinstance(resp.response_metadata, dict) else 'not a dict'}")
        
        if isinstance(resp.response_metadata, dict) and "grounding_metadata" in resp.response_metadata:
            grounding_metadata = resp.response_metadata["grounding_metadata"]
            logger.debug(f"    grounding_metadata keys: {list(grounding_metadata.keys()) if isinstance(grounding_metadata, dict) else 'not a dict'}")
            
            if isinstance(grounding_metadata, dict) and "grounding_chunks" in grounding_metadata:
                chunks = grounding_metadata["grounding_chunks"]
                logger.info(f"  ğŸ“‹ grounding_chunksæ•°: {len(chunks)}")
                for i, chunk in enumerate(chunks[:3], 1):  # æœ€åˆã®3å€‹ã®ã¿è©³ç´°ãƒ­ã‚°
                    logger.info(f"    [chunk {i}] keys: {list(chunk.keys()) if isinstance(chunk, dict) else 'not a dict'}")
                    if isinstance(chunk, dict) and "web" in chunk:
                        web_info = chunk["web"]
                        logger.info(f"      web keys: {list(web_info.keys()) if isinstance(web_info, dict) else 'not a dict'}")
                        if isinstance(web_info, dict):
                            logger.info(f"      web.uri: {web_info.get('uri', 'N/A')}")
                            # webã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
                            import json
                            logger.info(f"      webå…¨ä½“ (JSON): {json.dumps(web_info, ensure_ascii=False, indent=2)}")
        
        reference_urls = [
            chunk["web"]["uri"]
            for chunk in resp.response_metadata["grounding_metadata"]["grounding_chunks"]
        ]
        
        # å…¨ã¦ã®URLã‚’ãƒ­ã‚°ã«å‡ºåŠ›
        logger.info(f"  ğŸ“‹ å–å¾—ã—ãŸreference_urls ({len(reference_urls)}å€‹):")
        for i, url in enumerate(reference_urls, 1):
            logger.info(f"    {i}. {url}")
        
        # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‹ã‚‰å®Ÿéš›ã®URLã‚’æŠ½å‡ºï¼ˆtitleãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’å–å¾—ï¼‰
        direct_urls = []
        for i, chunk in enumerate(resp.response_metadata["grounding_metadata"]["grounding_chunks"]):
            uri = chunk["web"]["uri"]
            web_info = chunk["web"]
            
            if uri.startswith('https://vertexaisearch.cloud.google.com'):
                # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã®å ´åˆã€titleãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’å–å¾—
                if "title" in web_info and web_info["title"]:
                    domain = web_info["title"].strip()
                    # ãƒ‰ãƒ¡ã‚¤ãƒ³åã‹ã‚‰URLã‚’æ§‹ç¯‰
                    if domain and not domain.startswith('http'):
                        actual_url = f"https://{domain}"
                        direct_urls.append(actual_url)
                        logger.info(f"  âœ… ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‹ã‚‰æŠ½å‡ºï¼ˆtitleä½¿ç”¨ï¼‰: {actual_url}")
                    else:
                        logger.warning(f"  âš ï¸ titleãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒç„¡åŠ¹: {domain}")
                else:
                    logger.warning(f"  âš ï¸ titleãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {uri[:100]}")
            else:
                # ç›´æ¥URL
                direct_urls.append(uri)
                logger.debug(f"  ç›´æ¥URL: {uri}")
        
        if direct_urls:
            direct_count = len([u for u in reference_urls if not u.startswith('https://vertexaisearch.cloud.google.com')])
            redirect_extracted_count = len(direct_urls) - direct_count
            logger.info(f"  âœ… Googleæ¤œç´¢ã‹ã‚‰{len(direct_urls)}å€‹ã®URLå–å¾—ï¼ˆç›´æ¥: {direct_count}å€‹, ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‹ã‚‰æŠ½å‡º: {redirect_extracted_count}å€‹ï¼‰")
            urls.extend(direct_urls)
        else:
            logger.warning(f"  âš ï¸ Googleæ¤œç´¢çµæœã‹ã‚‰URLã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆ{len(reference_urls)}å€‹ï¼‰")
    except Exception as e:  # noqa: BLE001
        logger.error(f"  âŒ Googleæ¤œç´¢çµæœã®å‡¦ç†ã«å¤±æ•—: {type(e).__name__}: {str(e)}")
        import traceback
        logger.debug(f"  ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯: {traceback.format_exc()}")
    
    logger.info(f"  å–å¾—ã—ãŸURLå€™è£œ: {len(urls)}å€‹")

    # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³è¨­å®šã«åŸºã¥ã„ã¦å€™è£œURLã‚’ãƒ•ã‚£ãƒ«ã‚¿
    # ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚‚å«ã‚ã¦æœ«å°¾ä¸€è‡´ã§é™¤å¤–ã™ã‚‹
    def _is_excluded(url: str) -> bool:
        # EXCLUDE_DOMAINS ã¯æ”¹è¡ŒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ— or ãƒªã‚¹ãƒˆã‚’æƒ³å®š
        raw = settings.EXCLUDE_DOMAINS
        domains = [s.strip() for s in raw.splitlines() if s.strip()]
        return any(domain in url for domain in domains)

    filtered_urls = [u for u in urls if not _is_excluded(u)]
    excluded_count = len(urls) - len(filtered_urls)
    if excluded_count > 0:
        logger.info(f"  é™¤å¤–ã•ã‚ŒãŸURL: {excluded_count}å€‹")

    # åˆ°é”å¯èƒ½URLã«æ­£è¦åŒ–
    logger.info("  ğŸŒ URLåˆ°é”å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯ä¸­...")
    state.urls = convert_accessable_urls(filtered_urls)
    
    node_elapsed = time.time() - node_start
    logger.info(f"  âœ… æœ€çµ‚URLå€™è£œ: {len(state.urls)}å€‹")
    for i, url in enumerate(state.urls[:5], 1):  # æœ€å¤§5å€‹ã¾ã§è¡¨ç¤º
        logger.info(f"     {i}. {url}")
    if len(state.urls) > 5:
        logger.info(f"     ... ä»–{len(state.urls) - 5}å€‹")
    logger.info(f"  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: {node_elapsed:.2f}ç§’")
    
    return state


def node_select_official_website(state: ExtractState) -> ExtractState:
    """å€™è£œURLã‹ã‚‰å…¬å¼ã‚µã‚¤ãƒˆã‚’ä¸€ã¤é¸å®šã—ã¦çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹.

    Args:
        state: `urls` ã‚’å«ã‚€çŠ¶æ…‹ã€‚

    Returns:
        dict: `selected_url` ã‚’è¿½åŠ ã—ãŸçŠ¶æ…‹ã€‚

    """
    node_start = time.time()
    logger.info("-" * 60)
    logger.info("[NODE 2/3] node_select_official_website - å…¬å¼ã‚µã‚¤ãƒˆé¸å®š")
    logger.info(f"  å€™è£œURLæ•°: {len(state.urls)}å€‹")
    
    # URLå€™è£œãŒ1å€‹ä»¥ä¸‹ã®å ´åˆã€é¸å®šä¸è¦ï¼ˆæœ€é©åŒ–ï¼‰
    if len(state.urls) <= 1:
        logger.info("  â„¹ï¸ URLå€™è£œãŒ1å€‹ä»¥ä¸‹ã®ãŸã‚é¸å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        logger.info("  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: 0.00ç§’ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        return state
    
    urls = state.urls
    web_context = ""
    
    logger.info("  ğŸ•·ï¸ å„URLã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­ï¼ˆtimeout=20ç§’ï¼‰...")
    for i, url in enumerate(urls, 1):
        crawl_start = time.time()
        logger.info(f"     [{i}/{len(urls)}] {url}")
        markdown = crawl_markdown(url, timeout=20)
        crawl_elapsed = time.time() - crawl_start
        if not markdown:
            logger.warning(f"        âš ï¸ ã‚¯ãƒ­ãƒ¼ãƒ«å¤±æ•—ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({crawl_elapsed:.2f}ç§’)")
            continue  # å¤±æ•—ã—ãŸURLã¯ã‚¹ã‚­ãƒƒãƒ—
        logger.info(f"        âœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº† ({crawl_elapsed:.2f}ç§’, {len(markdown)}æ–‡å­—)")
        web_context += f"""# {url}\n{markdown}\n"""
    
    prompt = load_prompt(str(BASE_DIR / "agent/prompts/select_official.yaml"), encoding="utf-8")
    logger.debug("  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ãƒ¼ãƒ‰å®Œäº†")
    
    logger.info("  ğŸ¤– Gemini APIå‘¼ã³å‡ºã—ä¸­ï¼ˆå…¬å¼ã‚µã‚¤ãƒˆé¸å®šï¼‰...")
    api_start = time.time()
    
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash",
            temperature=0,
            google_api_key=settings.GOOGLE_API_KEY,
            max_retries=0,
        ).with_structured_output(URLScoreList)
        resp: URLScoreList = _invoke_with_retry(
            llm,
            prompt.format(company=state.company, location=state.location, web_context=web_context),
        )
        api_elapsed = time.time() - api_start
        
        # å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        actual_model = "ä¸æ˜"
        try:
            # with_structured_outputã‚’ä½¿ã£ã¦ã„ã‚‹å ´åˆã€å…ƒã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
            if hasattr(resp, 'response_metadata') and resp.response_metadata:
                metadata = resp.response_metadata
                if isinstance(metadata, dict):
                    actual_model = metadata.get('model_name', metadata.get('model', 'ä¸æ˜'))
                elif hasattr(metadata, 'model_name'):
                    actual_model = metadata.model_name
                elif hasattr(metadata, 'model'):
                    actual_model = metadata.model
        except Exception:
            pass
        
        logger.info(f"  âœ… APIå‘¼ã³å‡ºã—æˆåŠŸ ({api_elapsed:.2f}ç§’)")
        # with_structured_outputã‚’ä½¿ã£ã¦ã„ã‚‹å ´åˆã€å…ƒã®llmã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
        base_llm = llm if not hasattr(llm, 'llm') else llm.llm
        specified_model = getattr(base_llm, 'model', getattr(base_llm, 'model_name', 'gemini-2.0-flash'))
        logger.info(f"  ğŸ“Š ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: æŒ‡å®š={specified_model}, å®Ÿéš›={actual_model}")
        _wait_between_api_calls()  # APIå‘¼ã³å‡ºã—é–“ã®é–“éš”
    except Exception as e:
        api_elapsed = time.time() - api_start
        logger.error(f"  âŒ APIå‘¼ã³å‡ºã—å¤±æ•— ({api_elapsed:.2f}ç§’)")
        logger.error(f"  ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
        raise
    
    sorted_urls = sorted(resp.urls, key=lambda x: x.score, reverse=True)
    logger.info("  ğŸ“Š URLã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°çµæœ:")
    for i, url_score in enumerate(sorted_urls[:5], 1):
        logger.info(f"     {i}. {url_score.url} (ã‚¹ã‚³ã‚¢: {url_score.score})")

    def _is_excluded(url: str) -> bool:
        # EXCLUDE_DOMAINS ã¯æ”¹è¡ŒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ— or ãƒªã‚¹ãƒˆã‚’æƒ³å®š
        raw = settings.EXCLUDE_DOMAINS
        domains = [s.strip() for s in raw.splitlines() if s.strip()]
        return any(domain in url for domain in domains)

    state.urls = [url.url for url in sorted_urls if not _is_excluded(url.url)]
    
    node_elapsed = time.time() - node_start
    logger.info(f"  âœ… é¸å®šã•ã‚ŒãŸURL: {len(state.urls)}å€‹")
    logger.info(f"  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: {node_elapsed:.2f}ç§’")

    return state


def node_fetch_html(state: ExtractState) -> ExtractState:
    """é¸å®šURLã®HTMLã‚’å–å¾—ã—çŠ¶æ…‹ã«æ ¼ç´ã™ã‚‹.

    Args:
        state: `selected_url` ã‚’å«ã‚€çŠ¶æ…‹ã€‚

    Returns:
        dict: `html` ã‚’è¿½åŠ ã—ãŸçŠ¶æ…‹ï¼ˆå¤±æ•—æ™‚ã¯ç©ºæ–‡å­—ï¼‰ã€‚

    """
    node_start = time.time()
    logger.info("-" * 60)
    logger.info("[NODE 3/3] node_fetch_html - ä¼šç¤¾æƒ…å ±æŠ½å‡º")
    
    # URLå€™è£œãŒç„¡ã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹ï¼ˆValidationErrorã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
    if not state.urls:
        logger.warning("  âš ï¸ URLå€™è£œãŒ0å€‹ - æŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        raise ValueError("URLå€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ä¼šç¤¾æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã€‚")
    
    url = state.urls.pop(0)
    logger.info(f"  å¯¾è±¡URL: {url}")
    
    logger.info("  ğŸ•·ï¸ Webãƒšãƒ¼ã‚¸ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­ï¼ˆdepth=0, timeout=30ç§’ï¼‰...")
    crawl_start = time.time()
    try:
        # depth=0ã«å¤‰æ›´ï¼ˆãƒ‡ã‚£ãƒ¼ãƒ—ã‚¯ãƒ­ãƒ¼ãƒ«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã™ãã‚‹ãŸã‚ï¼‰
        web_context = crawl_markdown(url, depth=0, timeout=30)
        crawl_elapsed = time.time() - crawl_start
        if not web_context:
            logger.warning(f"  âš ï¸ ã‚¯ãƒ­ãƒ¼ãƒ«å¤±æ•—ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({crawl_elapsed:.2f}ç§’)")
            logger.warning(f"  âš ï¸ URL: {url}")
            raise ValueError(f"URL {url} ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ï¼‰ã€‚")
        logger.info(f"  âœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº† ({crawl_elapsed:.2f}ç§’, {len(web_context)}æ–‡å­—)")
    except Exception as e:
        crawl_elapsed = time.time() - crawl_start
        logger.error(f"  âŒ ã‚¯ãƒ­ãƒ¼ãƒ«ä¾‹å¤–ç™ºç”Ÿ ({crawl_elapsed:.2f}ç§’)")
        logger.error(f"  ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
        import traceback
        logger.debug(f"  ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯: {traceback.format_exc()}")
        raise ValueError(f"URL {url} ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ï¼‰ã€‚")

    prompt = load_prompt(str(BASE_DIR / "agent/prompts/extract_contact.yaml"), encoding="utf-8")
    logger.debug("  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ãƒ¼ãƒ‰å®Œäº†")
    
    logger.info("  ğŸ¤– Gemini APIå‘¼ã³å‡ºã—ä¸­ï¼ˆä¼šç¤¾æƒ…å ±æŠ½å‡ºï¼‰...")
    logger.info(f"     å¿…é ˆæ¥­ç¨®: {state.required_businesses}")
    logger.info(f"     å¿…é ˆã‚¸ãƒ£ãƒ³ãƒ«: {state.required_genre}")
    api_start = time.time()
    
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash",
            temperature=0,
            google_api_key=settings.GOOGLE_API_KEY,
            max_retries=0,
        ).with_structured_output(LLMCompanyInfo)
        resp: LLMCompanyInfo = _invoke_with_retry(
            llm,
            prompt.format(
                required_businesses=state.required_businesses,
                required_genre=state.required_genre,
                web_context=web_context,
            ),
        )
        api_elapsed = time.time() - api_start
        
        # å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        actual_model = "ä¸æ˜"
        try:
            # with_structured_outputã‚’ä½¿ã£ã¦ã„ã‚‹å ´åˆã€å…ƒã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
            if hasattr(resp, 'response_metadata') and resp.response_metadata:
                metadata = resp.response_metadata
                if isinstance(metadata, dict):
                    actual_model = metadata.get('model_name', metadata.get('model', 'ä¸æ˜'))
                elif hasattr(metadata, 'model_name'):
                    actual_model = metadata.model_name
                elif hasattr(metadata, 'model'):
                    actual_model = metadata.model
        except Exception:
            pass
        
        logger.info(f"  âœ… APIå‘¼ã³å‡ºã—æˆåŠŸ ({api_elapsed:.2f}ç§’)")
        # with_structured_outputã‚’ä½¿ã£ã¦ã„ã‚‹å ´åˆã€å…ƒã®llmã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
        base_llm = llm if not hasattr(llm, 'llm') else llm.llm
        specified_model = getattr(base_llm, 'model', getattr(base_llm, 'model_name', 'gemini-2.0-flash'))
        logger.info(f"  ğŸ“Š ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: æŒ‡å®š={specified_model}, å®Ÿéš›={actual_model}")
        # æœ€å¾Œã®APIå‘¼ã³å‡ºã—ãªã®ã§é–“éš”ã¯ä¸è¦
        logger.info("  ğŸ“‹ æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±:")
        logger.info(f"     ä¼šç¤¾å: {resp.company}")
        logger.info(f"     é›»è©±ç•ªå·: {resp.tel}")
        logger.info(f"     ä½æ‰€: {resp.address}")
        logger.info(f"     URL: {resp.url}")
        logger.info(f"     ãŠå•ã„åˆã‚ã›URL: {resp.contact_url}")
    except Exception as e:
        api_elapsed = time.time() - api_start
        logger.error(f"  âŒ APIå‘¼ã³å‡ºã—å¤±æ•— ({api_elapsed:.2f}ç§’)")
        logger.error(f"  ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
        raise

    # LLMã®å¿œç­”ã‚’è¾æ›¸ã«å¤‰æ›
    company_info_dict = resp.model_dump()
    
    # urlãŒNoneã®å ´åˆã¯ã€å®Ÿéš›ã«ã‚¯ãƒ­ãƒ¼ãƒ«ã—ãŸURLã‚’ä½¿ç”¨
    if not company_info_dict.get("url"):
        company_info_dict["url"] = url
        logger.info(f"  âš ï¸ LLMãŒurlã‚’æŠ½å‡ºã§ããªã‹ã£ãŸãŸã‚ã€ã‚¯ãƒ­ãƒ¼ãƒ«ã—ãŸURLã‚’ä½¿ç”¨: {url}")
    
    state.company_info = company_info_dict
    
    node_elapsed = time.time() - node_start
    logger.info(f"  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: {node_elapsed:.2f}ç§’")
    
    return state
