import json
import re
import time
from typing import Literal, Optional

from google import genai
from google.api_core.exceptions import ResourceExhausted
from google.genai import types as genai_types
from google.ai.generativelanguage_v1beta.types import Tool as GenAITool
from langchain_core.prompts.loading import load_prompt
from langchain_core.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from pydantic import BaseModel, Field, create_model, field_validator

from agent.state import ExtractState
from agent.tools import (
    get_check_url_accessibility_declaration,
    get_crawl_footer_links_declaration,
    get_crawl_website_declaration,
    get_report_company_info_declaration,
    get_report_url_scores_declaration,
    get_validate_company_info_declaration,
    handle_function_call,
)
from models.schemas import CompanyInfo, LLMCompanyInfo, URLScoreList
from models.settings import BASE_DIR, settings
from utils.crawl4ai_util import crawl_markdown
from utils.net import convert_accessable_urls
from utils.logger import get_logger

RETRY_DELAY_SECONDS = 10.0  # ResourceExhaustedæ™‚ã¯å¸¸ã«10ç§’å¾…æ©Ÿ
RETRY_ATTEMPTS = 9  # æœ€å¤§10å›è©¦è¡Œ (retries + 1)
API_CALL_INTERVAL_SECONDS = 5.0  # APIå‘¼ã³å‡ºã—é–“ã®é–“éš”ã‚’5ç§’ã«å¢—åŠ ï¼ˆå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ï¼‰
DEFAULT_GEMINI_MODEL = "gemini-2.5-flash"

logger = get_logger()


def _invoke_with_retry(operation, *, retries: int = RETRY_ATTEMPTS):
    """ä»»æ„ã®Gemini APIå‘¼ã³å‡ºã—ã‚’æœ€å¤§retrieså›å†è©¦è¡Œï¼ˆã‚¨ã‚¯ã‚¹ãƒãƒãƒ³ã‚·ãƒ£ãƒ«ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰ã§å®Ÿè¡Œ."""
    attempts = retries + 1
    for attempt in range(attempts):
        try:
            return operation()
        except ResourceExhausted as exc:
            if attempt == retries:
                error_msg = str(exc)
                logger.error(f"  âŒ ResourceExhaustedã‚¨ãƒ©ãƒ¼ï¼ˆæœ€çµ‚è©¦è¡Œå¤±æ•—ï¼‰: {error_msg[:300]}")
                if "quota" in error_msg.lower() or "limit" in error_msg.lower():
                    logger.error("  âš ï¸ ã‚¯ã‚©ãƒ¼ã‚¿/åˆ¶é™é–¢é€£ã®ã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                else:
                    logger.warning("  âš ï¸ ä¸€æ™‚çš„ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ˆã‚¯ã‚©ãƒ¼ã‚¿è¶…éã§ã¯ãªã„å¯èƒ½æ€§ï¼‰")
                raise
            backoff_delay = RETRY_DELAY_SECONDS
            logger.warning(
                "  âš ï¸ ResourceExhaustedã‚¨ãƒ©ãƒ¼ (attempt %s/%s). %sç§’å¾Œã«å†è©¦è¡Œã—ã¾ã™â€¦",
                attempt + 1,
                attempts,
                backoff_delay,
            )
            logger.debug(f"  ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(exc)[:200]}")
            time.sleep(backoff_delay)
        except Exception as exc:
            error_type = type(exc).__name__
            error_msg = str(exc)
            logger.error(f"  âŒ {error_type}ã‚¨ãƒ©ãƒ¼: {error_msg[:300]}")
            import traceback
            logger.debug(f"  ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯: {traceback.format_exc()}")
            raise


def _wait_between_api_calls():
    """APIå‘¼ã³å‡ºã—é–“ã®é–“éš”ã‚’ç©ºã‘ã‚‹."""
    logger.debug(f"  â³ APIå‘¼ã³å‡ºã—é–“éš”ã®ãŸã‚{API_CALL_INTERVAL_SECONDS}ç§’å¾…æ©Ÿä¸­...")
    time.sleep(API_CALL_INTERVAL_SECONDS)


def _load_json_from_text(text: str) -> Optional[dict]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æœ€åˆã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¾ãŸã¯é…åˆ—ã‚’æŠ½å‡ºã—ã¦è¾æ›¸ã«å¤‰æ›."""
    if not text:
        return None

    pattern_object = r"```json\s*(\{.*?\})\s*```"
    pattern_array = r"```json\s*(\[.*?\])\s*```"
    match = re.search(pattern_object, text, re.DOTALL) or re.search(pattern_array, text, re.DOTALL)
    json_candidate = None
    if match:
        json_candidate = match.group(1)
    else:
        start_idx = text.find("{")
        if start_idx == -1:
            start_idx = text.find("[")
        if start_idx != -1:
            brace = text[start_idx]
            stack = 0
            for i in range(start_idx, len(text)):
                char = text[i]
                if char == brace:
                    stack += 1
                elif (brace == "{" and char == "}") or (brace == "[" and char == "]"):
                    stack -= 1
                    if stack == 0:
                        json_candidate = text[start_idx : i + 1]
                        break
    if not json_candidate:
        return None
    try:
        return json.loads(json_candidate)
    except json.JSONDecodeError:
        return None


def _normalize_url_scores_payload(data: Optional[dict | list]) -> Optional[dict]:
    if data is None:
        return None
    if isinstance(data, list):
        return {"urls": data}
    if isinstance(data, dict) and "urls" in data:
        return data
    return None


def _create_gemini_client() -> genai.Client:
    return genai.Client(api_key=settings.GOOGLE_API_KEY)


def _build_user_content(text: str) -> genai_types.Content:
    return genai_types.Content(
        role="user",
        parts=[genai_types.Part.from_text(text=text)],
    )


def _content_to_text(content: Optional[genai_types.Content]) -> str:
    if not content:
        return ""
    parts = content.parts or []
    return "".join(part.text or "" for part in parts if part.text)


def _iter_function_calls(content: Optional[genai_types.Content]):
    if not content or not content.parts:
        return []
    return [
        part.function_call
        for part in content.parts
        if part.function_call is not None
    ]


def _append_function_response_message(
    messages: list[genai_types.Content], function_name: str, response_data: dict
):
    messages.append(
        genai_types.Content(
            role="tool",
            parts=[
                genai_types.Part.from_function_response(
                    name=function_name,
                    response=response_data,
                )
            ],
        )
    )


def node_get_url_candidates(state: ExtractState) -> ExtractState:
    """ä¼šç¤¾åãƒ»å‹¤å‹™åœ°ã‹ã‚‰URLå€™è£œã‚’æ¤œç´¢ã—ã¦çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹.

    Args:
        state: LangGraphã®çŠ¶æ…‹ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªï¼ˆ`company`, `location` ã‚’æƒ³å®šï¼‰ã€‚

    Returns:
        dict: `urls` ã‚­ãƒ¼ã«å€™è£œURLã®é…åˆ—ã‚’è¿½åŠ ã—ãŸæ–°ã—ã„çŠ¶æ…‹ã€‚

    """
    node_start = time.time()
    logger.info("-" * 60)
    logger.info("[NODE 1/3] node_get_url_candidates - URLå€™è£œã®å–å¾—")
    logger.info(f"  å…¥åŠ›: {state.company} @ {state.location}")
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’YAMLã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
    prompt = load_prompt(str(BASE_DIR / "agent/prompts/extract_url.yaml"), encoding="utf-8")
    logger.debug("  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ãƒ¼ãƒ‰å®Œäº†")

    # LLMï¼ˆæ¤œç´¢ãƒ„ãƒ¼ãƒ«æœ‰åŠ¹ï¼‰ã‚’å‘¼ã³å‡ºã—
    # max_retries=2ã«åˆ¶é™ã—ã¦ç„¡é™ãƒ«ãƒ¼ãƒ—ã‚’é˜²ã
    logger.info("  ğŸ¤– Gemini APIå‘¼ã³å‡ºã—ä¸­ï¼ˆGoogleæ¤œç´¢ãƒ„ãƒ¼ãƒ«æœ‰åŠ¹ï¼‰...")
    logger.info("  ğŸ” Google Searchãƒ„ãƒ¼ãƒ«ï¼ˆGrounding APIï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™")
    
    client = _create_gemini_client()
    google_search_tool = genai_types.Tool(google_search=genai_types.GoogleSearch())
    config = genai_types.GenerateContentConfig(
        tools=[google_search_tool],
        temperature=0,
    )
    prompt_text = prompt.format(company=state.company, location=state.location)

    urls: list[str] = []
    MAX_SEARCH_RETRIES = 3  # URLå–å¾—ã®ãƒªãƒˆãƒ©ã‚¤å›æ•°

    for attempt in range(MAX_SEARCH_RETRIES):
        logger.info(f"  ğŸ”„ æ¤œç´¢å®Ÿè¡Œ (è©¦è¡Œ {attempt + 1}/{MAX_SEARCH_RETRIES})")
        api_start = time.time()
        
        try:
            logger.info("  ğŸ”§ Google Searchãƒ„ãƒ¼ãƒ«ã‚’æœ‰åŠ¹åŒ–")
            resp = _invoke_with_retry(
                    lambda: client.models.generate_content(
                        model=DEFAULT_GEMINI_MODEL,
                    contents=[_build_user_content(prompt_text)],
                    config=config,
                )
            )
            api_elapsed = time.time() - api_start
            actual_model = getattr(resp, "model_version", "ä¸æ˜")
            logger.info(f"  âœ… APIå‘¼ã³å‡ºã—æˆåŠŸ ({api_elapsed:.2f}ç§’)")
            logger.info(f"  ğŸ“Š ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: å®Ÿéš›={actual_model}")

            logger.debug("  â³ Google Searchãƒ„ãƒ¼ãƒ«ä½¿ç”¨å¾Œã®è¿½åŠ å¾…æ©Ÿæ™‚é–“ï¼ˆ1ç§’ï¼‰...")
            time.sleep(1.0)
            _wait_between_api_calls()
        except Exception as e:
            api_elapsed = time.time() - api_start
            logger.error(f"  âŒ APIå‘¼ã³å‡ºã—å¤±æ•— ({api_elapsed:.2f}ç§’)")
            logger.error(f"  ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
            if attempt == MAX_SEARCH_RETRIES - 1:
                raise
            time.sleep(5.0) # ã‚¨ãƒ©ãƒ¼æ™‚ã®å¾…æ©Ÿ
            continue

        # å¿œç­”ã‹ã‚‰URLã‚’æŠ½å‡º
        candidate = resp.candidates[0] if resp.candidates else None
        resp_text = resp.text or _content_to_text(candidate.content if candidate else None)
        
        # ã¾ãšæœ¬æ–‡ã‹ã‚‰å…¨ã¦ã®URLã‚’æŠ½å‡ºï¼ˆæœ€ã‚‚ä¿¡é ¼æ€§ãŒé«˜ã„ï¼‰
        # ã‚ˆã‚Šå³å¯†ãªURLæ­£è¦è¡¨ç¾ã‚’ä½¿ç”¨ï¼ˆä¸å®Œå…¨ãªURLã‚’é™¤å¤–ï¼‰
        url_pattern = r'https?://[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*(?:/[^\s<>"]*)?'
        content_urls = re.findall(url_pattern, resp_text)
        
        # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã¨ä¸å®Œå…¨ãªURLã‚’é™¤å¤–
        def _is_valid_url(url: str) -> bool:
            """URLãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹."""
            # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‚’é™¤å¤–
            if 'grounding-api-redirect' in url:
                return False
            # ã‚¹ã‚­ãƒ¼ãƒ ã¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å«ã‚€å¿…è¦ãŒã‚ã‚‹
            if url.count('/') < 2:
                return False
            # ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ãƒ‰ãƒƒãƒˆã‚’å«ã‚€å¿…è¦ãŒã‚ã‚‹
            try:
                domain = url.split('//')[1].split('/')[0]
                if '.' not in domain:
                    return False
                # æ—¥æœ¬èªæ–‡å­—ã‚„å…¨è§’æ–‡å­—ã‚’å«ã‚€URLã‚’é™¤å¤–ï¼ˆä¸å®Œå…¨ãªæŠ½å‡ºã‚’é˜²ãï¼‰
                if any(ord(c) > 127 for c in url):
                    return False
            except (IndexError, AttributeError):
                return False
            return True
        
        content_urls = [url for url in content_urls if _is_valid_url(url)]
        
        if content_urls:
            urls.extend(content_urls)
            logger.info(f"  âœ… æœ¬æ–‡ã‹ã‚‰{len(content_urls)}å€‹ã®URLæŠ½å‡º:")
            for url in content_urls[:5]:
                logger.info(f"     - {url}")
        
        # groundingç”±æ¥URLï¼ˆãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‹ã‚‰å®Ÿéš›ã®URLã‚’æŠ½å‡ºï¼‰
        try:
            reference_urls = []
            if candidate and getattr(candidate, "grounding_metadata", None):
                grounding_metadata = candidate.grounding_metadata
                chunks = getattr(grounding_metadata, "grounding_chunks", None) or []
                logger.info(f"  ğŸ“‹ grounding_chunksæ•°: {len(chunks)}")
                for i, chunk in enumerate(chunks[:3], 1):
                    web_info = getattr(chunk, "web", None)
                    if web_info:
                        logger.info(f"    [chunk {i}] web.uri: {getattr(web_info, 'uri', 'N/A')}")
                reference_urls = [
                    getattr(chunk.web, "uri", "")
                    for chunk in chunks
                    if getattr(chunk, "web", None)
                ]
            
            # å…¨ã¦ã®URLã‚’ãƒ­ã‚°ã«å‡ºåŠ›
            logger.info(f"  ğŸ“‹ å–å¾—ã—ãŸreference_urls ({len(reference_urls)}å€‹):")
            for i, url in enumerate(reference_urls, 1):
                logger.info(f"    {i}. {url}")
            
            direct_urls = []
            if candidate and getattr(candidate, "grounding_metadata", None):
                chunks = getattr(candidate.grounding_metadata, "grounding_chunks", None) or []
                for chunk in chunks:
                    web_info = getattr(chunk, "web", None)
                    if not web_info:
                        continue
                    uri = getattr(web_info, "uri", "")
                    title = getattr(web_info, "title", "")
                    if uri.startswith("https://vertexaisearch.cloud.google.com") and title:
                        if not title.startswith("http"):
                            actual_url = f"https://{title.strip()}"
                        else:
                            actual_url = title.strip()
                        direct_urls.append(actual_url)
                        logger.info(f"  âœ… ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‹ã‚‰æŠ½å‡ºï¼ˆtitleä½¿ç”¨ï¼‰: {actual_url}")
                    else:
                        direct_urls.append(uri)
                        logger.debug(f"  ç›´æ¥URL: {uri}")
            
            if direct_urls:
                direct_count = len([u for u in reference_urls if not u.startswith('https://vertexaisearch.cloud.google.com')])
                redirect_extracted_count = len(direct_urls) - direct_count
                logger.info(f"  âœ… Googleæ¤œç´¢ã‹ã‚‰{len(direct_urls)}å€‹ã®URLå–å¾—ï¼ˆç›´æ¥: {direct_count}å€‹, ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‹ã‚‰æŠ½å‡º: {redirect_extracted_count}å€‹ï¼‰")
                urls.extend(direct_urls)
            else:
                logger.warning(f"  âš ï¸ Googleæ¤œç´¢çµæœã‹ã‚‰URLã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆ{len(reference_urls)}å€‹ï¼‰")
        except Exception as e:
            logger.error(f"  âŒ Googleæ¤œç´¢çµæœã®å‡¦ç†ã«å¤±æ•—: {type(e).__name__}: {str(e)}")
            import traceback
            logger.debug(f"  ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯: {traceback.format_exc()}")
        
        # URLãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
        if urls:
            break
        
        logger.warning(f"  âš ï¸ URLå€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ (è©¦è¡Œ {attempt + 1}/{MAX_SEARCH_RETRIES})")
        if attempt < MAX_SEARCH_RETRIES - 1:
            logger.info("  ğŸ”„ å†æ¤œç´¢ã®ãŸã‚å¾…æ©Ÿä¸­...")
            time.sleep(2.0)

    logger.info(f"  å–å¾—ã—ãŸURLå€™è£œ: {len(urls)}å€‹")

    # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³è¨­å®šã«åŸºã¥ã„ã¦å€™è£œURLã‚’ãƒ•ã‚£ãƒ«ã‚¿
    # ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚‚å«ã‚ã¦æœ«å°¾ä¸€è‡´ã§é™¤å¤–ã™ã‚‹
    def _is_excluded(url: str) -> bool:
        # EXCLUDE_DOMAINS ã¯æ”¹è¡ŒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ— or ãƒªã‚¹ãƒˆã‚’æƒ³å®š
        raw = settings.EXCLUDE_DOMAINS
        domains = [s.strip() for s in raw.splitlines() if s.strip()]
        return any(domain in url for domain in domains)

    filtered_urls = [u for u in urls if not _is_excluded(u)]
    excluded_count = len(urls) - len(filtered_urls)
    if excluded_count > 0:
        logger.info(f"  é™¤å¤–ã•ã‚ŒãŸURL: {excluded_count}å€‹")

    # åˆ°é”å¯èƒ½URLã«æ­£è¦åŒ–
    logger.info("  ğŸŒ URLåˆ°é”å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯ä¸­...")
    state.urls = convert_accessable_urls(filtered_urls)
    
    node_elapsed = time.time() - node_start
    logger.info(f"  âœ… æœ€çµ‚URLå€™è£œ: {len(state.urls)}å€‹")
    for i, url in enumerate(state.urls[:5], 1):  # æœ€å¤§5å€‹ã¾ã§è¡¨ç¤º
        logger.info(f"     {i}. {url}")
    if len(state.urls) > 5:
        logger.info(f"     ... ä»–{len(state.urls) - 5}å€‹")
    logger.info(f"  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: {node_elapsed:.2f}ç§’")
    
    return state


def node_select_official_website(state: ExtractState) -> ExtractState:
    """å€™è£œURLã‹ã‚‰å…¬å¼ã‚µã‚¤ãƒˆã‚’ä¸€ã¤é¸å®šã—ã¦çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹.

    Args:
        state: `urls` ã‚’å«ã‚€çŠ¶æ…‹ã€‚

    Returns:
        dict: `selected_url` ã‚’è¿½åŠ ã—ãŸçŠ¶æ…‹ã€‚

    """
    node_start = time.time()
    logger.info("-" * 60)
    logger.info("[NODE 2/3] node_select_official_website - å…¬å¼ã‚µã‚¤ãƒˆé¸å®š")
    logger.info(f"  å€™è£œURLæ•°: {len(state.urls)}å€‹")
    
    # URLå€™è£œãŒ1å€‹ä»¥ä¸‹ã®å ´åˆã€é¸å®šä¸è¦ï¼ˆæœ€é©åŒ–ï¼‰
    if len(state.urls) <= 1:
        logger.info("  â„¹ï¸ URLå€™è£œãŒ1å€‹ä»¥ä¸‹ã®ãŸã‚é¸å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        logger.info("  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: 0.00ç§’ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        return state
    
    urls = state.urls
    web_context = ""
    
    logger.info(f"  ğŸ•·ï¸ å„URLã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­ï¼ˆtimeout=10ç§’, è¨ˆ{len(urls)}ä»¶ï¼‰...")
    for i, url in enumerate(urls, 1):
        crawl_start = time.time()
        logger.info(f"     [{i}/{len(urls)}] {url}")
        markdown = crawl_markdown(url, timeout=10)  # 20ç§’ â†’ 10ç§’ã«çŸ­ç¸®
        crawl_elapsed = time.time() - crawl_start
        if not markdown:
            logger.warning(f"        âš ï¸ ã‚¯ãƒ­ãƒ¼ãƒ«å¤±æ•—ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({crawl_elapsed:.2f}ç§’)")
            continue  # å¤±æ•—ã—ãŸURLã¯ã‚¹ã‚­ãƒƒãƒ—
        logger.info(f"        âœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº† ({crawl_elapsed:.2f}ç§’, {len(markdown)}æ–‡å­—)")
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’åˆ¶é™ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ï¼‰
        # å„URLã®ã‚¯ãƒ­ãƒ¼ãƒ«çµæœã‚’10,000æ–‡å­—ã¾ã§ã«åˆ¶é™
        if len(markdown) > 10000:
            markdown = markdown[:10000]
            logger.debug(f"        âš ï¸ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’åˆ¶é™: {len(markdown)}æ–‡å­—ï¼ˆ10,000æ–‡å­—ã¾ã§ï¼‰")
        web_context += f"""# {url}\n{markdown}\n"""
    if not web_context:
        logger.warning("  âš ï¸ Webã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®ãŸã‚ã€å…¬å¼ã‚µã‚¤ãƒˆé¸å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        raise ValueError("å€™è£œURLã®ã‚¯ãƒ­ãƒ¼ãƒ«ã«ã™ã¹ã¦å¤±æ•—ã—ã€Webã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    prompt = load_prompt(str(BASE_DIR / "agent/prompts/select_official.yaml"), encoding="utf-8")
    logger.debug("  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ãƒ¼ãƒ‰å®Œäº†")
    
    logger.info("  ğŸ¤– Gemini APIå‘¼ã³å‡ºã—ä¸­ï¼ˆå…¬å¼ã‚µã‚¤ãƒˆé¸å®šï¼‰...")
    api_start = time.time()
    client = _create_gemini_client()
    tools = [
        genai_types.Tool(
            function_declarations=[
                get_crawl_website_declaration(),
                get_crawl_footer_links_declaration(),
                get_report_url_scores_declaration(),
            ]
        )
    ]
    config = genai_types.GenerateContentConfig(
        tools=tools,
        temperature=0,
    )
    messages = [
        _build_user_content(
            prompt.format(
                company=state.company,
                location=state.location,
                web_context=web_context,
            )
        )
    ]
    MAX_REPORT_RETRIES = 2
    max_iterations = MAX_REPORT_RETRIES + 1  # åˆå› + æœ€å¤§2å›ã®ãƒªãƒˆãƒ©ã‚¤
    url_score_payload: Optional[dict] = None
    report_retry_count = 0
    
    try:
        for iteration in range(max_iterations):
            resp = _invoke_with_retry(
                lambda: client.models.generate_content(
                    model=DEFAULT_GEMINI_MODEL,
                    contents=messages,
                    config=config,
                )
            )
            candidate = resp.candidates[0] if resp.candidates else None
            if not candidate:
                raise ValueError("LLMã®å¿œç­”ãŒç©ºã§ã—ãŸã€‚")
            tool_calls = _iter_function_calls(candidate.content)
            messages.append(candidate.content)
            
            # report_url_scores ãŒç”Ÿæˆã•ã‚ŒãŸã‚‰ãã‚Œã‚’æ¡ç”¨
            report_call = next((fc for fc in tool_calls if fc.name == "report_url_scores"), None)
            if report_call:
                url_score_payload = _normalize_url_scores_payload(dict(report_call.args or {}))
                break
            
            fallback_handled = False
            if not tool_calls:
                fallback_json = _load_json_from_text(_content_to_text(candidate.content))
                url_score_payload = _normalize_url_scores_payload(fallback_json)
                if url_score_payload:
                    break
                fallback_handled = True
            
            if tool_calls and not fallback_handled:
                for fc in tool_calls:
                    if fc.name == "report_url_scores":
                        continue
                    result = handle_function_call(fc.name, dict(fc.args or {}))
                    _append_function_response_message(messages, fc.name, result)
                _wait_between_api_calls()

            if url_score_payload:
                break

            if iteration < max_iterations - 1:
                report_retry_count += 1
                reminder = _build_user_content(
                    "ä¸Šè¨˜ã®æƒ…å ±ã‚’è¸ã¾ãˆã¦å…¬å¼ã‚µã‚¤ãƒˆã®ã¿ã‚’å¯¾è±¡ã¨ã—ã€å¿…ãš `report_url_scores` é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ã‚¹ã‚³ã‚¢ä»˜ãURLä¸€è¦§ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
                )
                messages.append(reminder)
                logger.info(f"  ğŸ” report_url_scoresã®å†ãƒªã‚¯ã‚¨ã‚¹ãƒˆ ({report_retry_count}/{MAX_REPORT_RETRIES})")

        else:
            logger.error("  âŒ report_url_scoresãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ã‚’å‡ºåŠ›
            raw_response = _content_to_text(candidate.content)
            logger.error(f"  ğŸ” LLMç”Ÿå¿œç­” (å…ˆé ­1000æ–‡å­—): {raw_response[:1000]}")
            raise ValueError("LLMãŒURLã‚¹ã‚³ã‚¢ã‚’å‡ºåŠ›ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    except Exception as e:
        api_elapsed = time.time() - api_start
        logger.error(f"  âŒ APIå‘¼ã³å‡ºã—å¤±æ•— ({api_elapsed:.2f}ç§’)")
        logger.error(f"  ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
        raise
    
    if not url_score_payload:
        raise ValueError("LLMã®å¿œç­”ã‹ã‚‰URLã‚¹ã‚³ã‚¢ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    resp_scores = URLScoreList.model_validate(url_score_payload)
    sorted_urls = sorted(resp_scores.urls, key=lambda x: x.score, reverse=True)
    logger.info("  ğŸ“Š URLã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°çµæœ:")
    for i, url_score in enumerate(sorted_urls[:5], 1):
        logger.info(f"     {i}. {url_score.url} (ã‚¹ã‚³ã‚¢: {url_score.score})")

    def _is_excluded(url: str) -> bool:
        # EXCLUDE_DOMAINS ã¯æ”¹è¡ŒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ— or ãƒªã‚¹ãƒˆã‚’æƒ³å®š
        raw = settings.EXCLUDE_DOMAINS
        domains = [s.strip() for s in raw.splitlines() if s.strip()]
        return any(domain in url for domain in domains)

    state.urls = [url.url for url in sorted_urls if not _is_excluded(url.url)]
    
    node_elapsed = time.time() - node_start
    logger.info(f"  âœ… é¸å®šã•ã‚ŒãŸURL: {len(state.urls)}å€‹")
    logger.info(f"  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: {node_elapsed:.2f}ç§’")

    return state


def _split_text_into_chunks(text: str, chunk_size: int = 8000) -> list[str]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã—ãŸæ–‡å­—æ•°ã§åˆ†å‰²ã™ã‚‹."""
    if not text:
        return [""]
    return [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]


def node_fetch_html(state: ExtractState) -> ExtractState:
    """é¸å®šURLã®HTMLã‚’å–å¾—ã—çŠ¶æ…‹ã«æ ¼ç´ã™ã‚‹.

    Args:
        state: `selected_url` ã‚’å«ã‚€çŠ¶æ…‹ã€‚

    Returns:
        dict: `html` ã‚’è¿½åŠ ã—ãŸçŠ¶æ…‹ï¼ˆå¤±æ•—æ™‚ã¯ç©ºæ–‡å­—ï¼‰ã€‚

    """
    node_start = time.time()
    logger.info("-" * 60)
    logger.info("[NODE 3/3] node_fetch_html - ä¼šç¤¾æƒ…å ±æŠ½å‡º")
    
    # URLå€™è£œãŒç„¡ã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹ï¼ˆValidationErrorã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
    if not state.urls:
        logger.warning("  âš ï¸ URLå€™è£œãŒ0å€‹ - æŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        raise ValueError("URLå€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ä¼šç¤¾æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã€‚")
    
    url = state.urls.pop(0)
    logger.info(f"  å¯¾è±¡URL: {url}")
    
    logger.info("  ğŸ•·ï¸ Webãƒšãƒ¼ã‚¸ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­ï¼ˆdepth=0, timeout=10ç§’ï¼‰...")
    crawl_start = time.time()
    try:
        # depth=0ã«å¤‰æ›´ï¼ˆãƒ‡ã‚£ãƒ¼ãƒ—ã‚¯ãƒ­ãƒ¼ãƒ«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã™ãã‚‹ãŸã‚ï¼‰
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’10ç§’ã«çŸ­ç¸®ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ï¼‰
        web_context = crawl_markdown(url, depth=0, timeout=10)  # 30ç§’ â†’ 10ç§’ã«çŸ­ç¸®
        crawl_elapsed = time.time() - crawl_start
        if not web_context:
            logger.warning(f"  âš ï¸ ã‚¯ãƒ­ãƒ¼ãƒ«å¤±æ•—ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({crawl_elapsed:.2f}ç§’)")
            logger.warning(f"  âš ï¸ URL: {url}")
            raise ValueError(f"URL {url} ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ï¼‰ã€‚")
        logger.info(f"  âœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº† ({crawl_elapsed:.2f}ç§’, {len(web_context)}æ–‡å­—)")
    except Exception as e:
        crawl_elapsed = time.time() - crawl_start
        logger.error(f"  âŒ ã‚¯ãƒ­ãƒ¼ãƒ«ä¾‹å¤–ç™ºç”Ÿ ({crawl_elapsed:.2f}ç§’)")
        logger.error(f"  ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
        import traceback
        logger.debug(f"  ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯: {traceback.format_exc()}")
        raise ValueError(f"URL {url} ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ï¼‰ã€‚")

    # æ§‹é€ åŒ–å‡ºåŠ›ã§ã¯å…¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€åº¦ã«å‡¦ç†
    # web_context_chunks = _split_text_into_chunks(web_context, chunk_size=8000)
    # current_chunk_index = 0
    
    prompt = load_prompt(str(BASE_DIR / "agent/prompts/extract_contact.yaml"), encoding="utf-8")
    logger.debug("  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ãƒ¼ãƒ‰å®Œäº†")
    
    logger.info("  ğŸ¤– Gemini APIå‘¼ã³å‡ºã—ä¸­ï¼ˆä¼šç¤¾æƒ…å ±æŠ½å‡ºï¼‰...")
    logger.info(f"     å¿…é ˆæ¥­ç¨®: {state.required_businesses}")
    logger.info(f"     å¿…é ˆã‚¸ãƒ£ãƒ³ãƒ«: {state.required_genre}")
    api_start = time.time()
    
    # æ§‹é€ åŒ–å‡ºåŠ›ç”¨ã®Pydanticãƒ¢ãƒ‡ãƒ«å®šç¾©
    class StructuredCompanyInfo(BaseModel):
        """æ§‹é€ åŒ–å‡ºåŠ›ç”¨ã®ä¼šç¤¾æƒ…å ±ãƒ¢ãƒ‡ãƒ«"""
        company: Optional[str] = Field(None, description="ä¼šç¤¾åã€‚æ ªå¼ä¼šç¤¾/æœ‰é™ä¼šç¤¾ç­‰ã‚’å«ã‚€æ­£å¼åç§°")
        tel: Optional[str] = Field(None, description="é›»è©±ç•ªå·ã€‚åŠè§’æ•°å­—ã¨ãƒã‚¤ãƒ•ãƒ³ã®ã¿ã®å½¢å¼")
        address: Optional[str] = Field(None, description="ä½æ‰€ã€‚éƒ½é“åºœçœŒã‚’å«ã‚€å®Œå…¨ãªä½æ‰€")
        first_name: Optional[str] = Field(None, description="æ‹…å½“è€…åãƒ»ä»£è¡¨è€…å")
        url: Optional[str] = Field(None, description="å…¬å¼ã‚µã‚¤ãƒˆã®URL")
        contact_url: Optional[str] = Field(None, description="ãŠå•ã„åˆã‚ã›ãƒšãƒ¼ã‚¸ã®URL")

    prompt_content = prompt.format(
        web_context=web_context,
        chunk_info="å®Œå…¨ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„",
    )
    
    client = _create_gemini_client()
    

    config = genai_types.GenerateContentConfig(
        response_mime_type="application/json",
        response_schema=StructuredCompanyInfo.model_json_schema(),
        temperature=0.3,
    )
    messages = [_build_user_content(prompt_content)]
    
    try:
        resp = _invoke_with_retry(
            lambda: client.models.generate_content(
                model=DEFAULT_GEMINI_MODEL,
                contents=messages,
                config=config,
            )
        )
        # æ§‹é€ åŒ–å‡ºåŠ›ã‹ã‚‰ç›´æ¥ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        candidate = resp.candidates[0] if resp.candidates else None
        if not candidate:
            raise ValueError("LLMã®å¿œç­”ãŒç©ºã§ã—ãŸã€‚")
        
        response_text = _content_to_text(candidate.content)
        company_info_payload = json.loads(response_text)
        
    except Exception as e:
        api_elapsed = time.time() - api_start
        logger.error(f"  âŒ APIå‘¼ã³å‡ºã—å¤±æ•— ({api_elapsed:.2f}ç§’)")
        logger.error(f"  ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
        raise
    
    if not company_info_payload:
        raise ValueError("LLMã®å¿œç­”ã‹ã‚‰ä¼šç¤¾æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    resp_info = StructuredCompanyInfo.model_validate(company_info_payload)
    logger.info("  âœ… ä¼šç¤¾æƒ…å ±ã®æ§‹é€ åŒ–ã«æˆåŠŸ")
    logger.info(f"     ä¼šç¤¾å: {resp_info.company}")
    logger.info(f"     é›»è©±ç•ªå·: {resp_info.tel}")
    logger.info(f"     ä½æ‰€: {resp_info.address}")
    logger.info(f"     URL: {resp_info.url}")
    logger.info(f"     ãŠå•ã„åˆã‚ã›URL: {resp_info.contact_url}")
    # æ¥­ç¨®ãƒ»ã‚¸ãƒ£ãƒ³ãƒ«ã¯ã¾ã ãªã„ã®ã§å‡ºåŠ›ã—ãªã„ï¼ˆå¾Œã§è¿½åŠ ï¼‰
    
    company_info_dict = resp_info.model_dump()
    
    # urlãŒNoneã®å ´åˆã¯ã€å®Ÿéš›ã«ã‚¯ãƒ­ãƒ¼ãƒ«ã—ãŸURLã‚’ä½¿ç”¨
    if not company_info_dict.get("url"):
        company_info_dict["url"] = url
        logger.info(f"  âš ï¸ LLMãŒurlã‚’æŠ½å‡ºã§ããªã‹ã£ãŸãŸã‚ã€ã‚¯ãƒ­ãƒ¼ãƒ«ã—ãŸURLã‚’ä½¿ç”¨: {url}")
    
    # businessã¨genreã¯å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®required_businessesã¨required_genreã‹ã‚‰å–å¾—ï¼ˆLLMã®èª¤æŠ½å‡ºã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
    if state.required_businesses and len(state.required_businesses) > 0:
        company_info_dict["business"] = state.required_businesses[0]
        logger.info(f"  ğŸ“ businessã‚’å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—: {company_info_dict['business']}")
    else:
        company_info_dict["business"] = ""
        logger.info("  ğŸ“ businessã‚’å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆç©ºæ–‡å­—ã‚’è¨­å®šï¼‰")
    
    if state.required_genre and len(state.required_genre) > 0:
        company_info_dict["genre"] = state.required_genre[0]
        logger.info(f"  ğŸ“ genreã‚’å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—: {company_info_dict['genre']}")
    else:
        company_info_dict["genre"] = ""
        logger.info("  ğŸ“ genreã‚’å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆç©ºæ–‡å­—ã‚’è¨­å®šï¼‰")
    
    state.company_info = company_info_dict
    
    node_elapsed = time.time() - node_start
    logger.info(f"  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: {node_elapsed:.2f}ç§’")
    
    return state
