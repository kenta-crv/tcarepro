import re
import time

from google.ai.generativelanguage_v1beta.types import Tool as GenAITool
from google.api_core.exceptions import ResourceExhausted
from langchain_core.prompts.loading import load_prompt
from langchain_google_genai import ChatGoogleGenerativeAI

from agent.state import ExtractState
from models.schemas import CompanyInfo, LLMCompanyInfo, URLScoreList
from models.settings import BASE_DIR, settings
from utils.crawl4ai_util import crawl_markdown
from utils.net import convert_accessable_urls
from utils.logger import get_logger

RETRY_DELAY_SECONDS = 4.0
RETRY_ATTEMPTS = 1  # 1å›ãƒªãƒˆãƒ©ã‚¤ = æœ€å¤§2å›è©¦è¡Œ

logger = get_logger()


def _invoke_with_retry(llm, prompt_str: str, *, retries: int = RETRY_ATTEMPTS, **invoke_kwargs):
    """Gemini APIå‘¼ã³å‡ºã—ã‚’æœ€å¤§retrieså›å†è©¦è¡Œï¼ˆ4ç§’å¾…æ©Ÿï¼‰ã§å®Ÿè¡Œ."""
    attempts = retries + 1
    for attempt in range(attempts):
        try:
            return llm.invoke(prompt_str, **invoke_kwargs)
        except ResourceExhausted as exc:
            if attempt == retries:
                raise
            logger.warning(
                "  âš ï¸ Gemini APIã‚¯ã‚©ãƒ¼ã‚¿è¶…é (attempt %s/%s). %sç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™â€¦",
                attempt + 1,
                attempts,
                RETRY_DELAY_SECONDS,
            )
            time.sleep(RETRY_DELAY_SECONDS)
        except Exception:
            raise


def node_get_url_candidates(state: ExtractState) -> ExtractState:
    """ä¼šç¤¾åãƒ»å‹¤å‹™åœ°ã‹ã‚‰URLå€™è£œã‚’æ¤œç´¢ã—ã¦çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹.

    Args:
        state: LangGraphã®çŠ¶æ…‹ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªï¼ˆ`company`, `location` ã‚’æƒ³å®šï¼‰ã€‚

    Returns:
        dict: `urls` ã‚­ãƒ¼ã«å€™è£œURLã®é…åˆ—ã‚’è¿½åŠ ã—ãŸæ–°ã—ã„çŠ¶æ…‹ã€‚

    """
    node_start = time.time()
    logger.info("-" * 60)
    logger.info("[NODE 1/3] node_get_url_candidates - URLå€™è£œã®å–å¾—")
    logger.info(f"  å…¥åŠ›: {state.company} @ {state.location}")
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’YAMLã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
    prompt = load_prompt(str(BASE_DIR / "agent/prompts/extract_url.yaml"), encoding="utf-8")
    logger.debug("  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ãƒ¼ãƒ‰å®Œäº†")

    # LLMï¼ˆæ¤œç´¢ãƒ„ãƒ¼ãƒ«æœ‰åŠ¹ï¼‰ã‚’å‘¼ã³å‡ºã—
    # max_retries=2ã«åˆ¶é™ã—ã¦ç„¡é™ãƒ«ãƒ¼ãƒ—ã‚’é˜²ã
    logger.info("  ğŸ¤– Gemini APIå‘¼ã³å‡ºã—ä¸­ï¼ˆGoogleæ¤œç´¢ãƒ„ãƒ¼ãƒ«æœ‰åŠ¹ï¼‰...")
    api_start = time.time()
    
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-lite",
            temperature=0,
            google_api_key=settings.GOOGLE_API_KEY,
            max_retries=0,
        )
        resp = _invoke_with_retry(
            llm,
            prompt.format(company=state.company, location=state.location),
            tools=[GenAITool(google_search={})],
        )
        api_elapsed = time.time() - api_start
        logger.info(f"  âœ… APIå‘¼ã³å‡ºã—æˆåŠŸ ({api_elapsed:.2f}ç§’)")
    except Exception as e:
        api_elapsed = time.time() - api_start
        logger.error(f"  âŒ APIå‘¼ã³å‡ºã—å¤±æ•— ({api_elapsed:.2f}ç§’)")
        logger.error(f"  ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
        raise

    # å¿œç­”ã‹ã‚‰URLã‚’æŠ½å‡º
    urls: list[str] = []
    
    # ã¾ãšæœ¬æ–‡ã‹ã‚‰å…¨ã¦ã®URLã‚’æŠ½å‡ºï¼ˆæœ€ã‚‚ä¿¡é ¼æ€§ãŒé«˜ã„ï¼‰
    # ã‚ˆã‚Šå³å¯†ãªURLæ­£è¦è¡¨ç¾ã‚’ä½¿ç”¨ï¼ˆä¸å®Œå…¨ãªURLã‚’é™¤å¤–ï¼‰
    url_pattern = r'https?://[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*(?:/[^\s<>"]*)?'
    content_urls = re.findall(url_pattern, resp.content)
    # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã¨ä¸å®Œå…¨ãªURLã‚’é™¤å¤–
    def _is_valid_url(url: str) -> bool:
        """URLãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹."""
        # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‚’é™¤å¤–
        if 'grounding-api-redirect' in url:
            return False
        # ã‚¹ã‚­ãƒ¼ãƒ ã¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å«ã‚€å¿…è¦ãŒã‚ã‚‹
        if url.count('/') < 2:
            return False
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ãƒ‰ãƒƒãƒˆã‚’å«ã‚€å¿…è¦ãŒã‚ã‚‹
        try:
            domain = url.split('//')[1].split('/')[0]
            if '.' not in domain:
                return False
            # æ—¥æœ¬èªæ–‡å­—ã‚„å…¨è§’æ–‡å­—ã‚’å«ã‚€URLã‚’é™¤å¤–ï¼ˆä¸å®Œå…¨ãªæŠ½å‡ºã‚’é˜²ãï¼‰
            if any(ord(c) > 127 for c in url):
                return False
        except (IndexError, AttributeError):
            return False
        return True
    
    content_urls = [url for url in content_urls if _is_valid_url(url)]
    
    if content_urls:
        urls.extend(content_urls)
        logger.info(f"  âœ… æœ¬æ–‡ã‹ã‚‰{len(content_urls)}å€‹ã®URLæŠ½å‡º:")
        for url in content_urls[:5]:
            logger.info(f"     - {url}")
    
    # groundingç”±æ¥URLï¼ˆãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã¯ä½¿ã‚ãªã„ï¼‰
    try:
        reference_urls = [
            chunk["web"]["uri"]
            for chunk in resp.response_metadata["grounding_metadata"]["grounding_chunks"]
        ]
        # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‚’é™¤å¤–
        direct_urls = [url for url in reference_urls if not url.startswith('https://vertexaisearch.cloud.google.com')]
        if direct_urls:
            logger.info(f"  âœ… Googleæ¤œç´¢ã‹ã‚‰{len(direct_urls)}å€‹ã®ç›´æ¥URLå–å¾—")
            urls.extend(direct_urls)
        else:
            logger.warning(f"  âš ï¸ Googleæ¤œç´¢çµæœã¯å…¨ã¦ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLï¼ˆ{len(reference_urls)}å€‹ï¼‰- ã‚¹ã‚­ãƒƒãƒ—")
    except Exception:  # noqa: BLE001
        logger.warning("  âš ï¸ Googleæ¤œç´¢çµæœãªã—")
    
    logger.info(f"  å–å¾—ã—ãŸURLå€™è£œ: {len(urls)}å€‹")

    # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³è¨­å®šã«åŸºã¥ã„ã¦å€™è£œURLã‚’ãƒ•ã‚£ãƒ«ã‚¿
    # ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚‚å«ã‚ã¦æœ«å°¾ä¸€è‡´ã§é™¤å¤–ã™ã‚‹
    def _is_excluded(url: str) -> bool:
        # EXCLUDE_DOMAINS ã¯æ”¹è¡ŒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ— or ãƒªã‚¹ãƒˆã‚’æƒ³å®š
        raw = settings.EXCLUDE_DOMAINS
        domains = [s.strip() for s in raw.splitlines() if s.strip()]
        return any(domain in url for domain in domains)

    filtered_urls = [u for u in urls if not _is_excluded(u)]
    excluded_count = len(urls) - len(filtered_urls)
    if excluded_count > 0:
        logger.info(f"  é™¤å¤–ã•ã‚ŒãŸURL: {excluded_count}å€‹")

    # åˆ°é”å¯èƒ½URLã«æ­£è¦åŒ–
    logger.info("  ğŸŒ URLåˆ°é”å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯ä¸­...")
    state.urls = convert_accessable_urls(filtered_urls)
    
    node_elapsed = time.time() - node_start
    logger.info(f"  âœ… æœ€çµ‚URLå€™è£œ: {len(state.urls)}å€‹")
    for i, url in enumerate(state.urls[:5], 1):  # æœ€å¤§5å€‹ã¾ã§è¡¨ç¤º
        logger.info(f"     {i}. {url}")
    if len(state.urls) > 5:
        logger.info(f"     ... ä»–{len(state.urls) - 5}å€‹")
    logger.info(f"  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: {node_elapsed:.2f}ç§’")
    
    return state


def node_select_official_website(state: ExtractState) -> ExtractState:
    """å€™è£œURLã‹ã‚‰å…¬å¼ã‚µã‚¤ãƒˆã‚’ä¸€ã¤é¸å®šã—ã¦çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹.

    Args:
        state: `urls` ã‚’å«ã‚€çŠ¶æ…‹ã€‚

    Returns:
        dict: `selected_url` ã‚’è¿½åŠ ã—ãŸçŠ¶æ…‹ã€‚

    """
    node_start = time.time()
    logger.info("-" * 60)
    logger.info("[NODE 2/3] node_select_official_website - å…¬å¼ã‚µã‚¤ãƒˆé¸å®š")
    logger.info(f"  å€™è£œURLæ•°: {len(state.urls)}å€‹")
    
    # URLå€™è£œãŒ1å€‹ä»¥ä¸‹ã®å ´åˆã€é¸å®šä¸è¦ï¼ˆæœ€é©åŒ–ï¼‰
    if len(state.urls) <= 1:
        logger.info("  â„¹ï¸ URLå€™è£œãŒ1å€‹ä»¥ä¸‹ã®ãŸã‚é¸å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        logger.info("  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: 0.00ç§’ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        return state
    
    urls = state.urls
    web_context = ""
    
    logger.info("  ğŸ•·ï¸ å„URLã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­ï¼ˆtimeout=20ç§’ï¼‰...")
    for i, url in enumerate(urls, 1):
        crawl_start = time.time()
        logger.info(f"     [{i}/{len(urls)}] {url}")
        markdown = crawl_markdown(url, timeout=20)
        crawl_elapsed = time.time() - crawl_start
        if not markdown:
            logger.warning(f"        âš ï¸ ã‚¯ãƒ­ãƒ¼ãƒ«å¤±æ•—ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({crawl_elapsed:.2f}ç§’)")
            continue  # å¤±æ•—ã—ãŸURLã¯ã‚¹ã‚­ãƒƒãƒ—
        logger.info(f"        âœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº† ({crawl_elapsed:.2f}ç§’, {len(markdown)}æ–‡å­—)")
        web_context += f"""# {url}\n{markdown}\n"""
    
    prompt = load_prompt(str(BASE_DIR / "agent/prompts/select_official.yaml"), encoding="utf-8")
    logger.debug("  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ãƒ¼ãƒ‰å®Œäº†")
    
    logger.info("  ğŸ¤– Gemini APIå‘¼ã³å‡ºã—ä¸­ï¼ˆå…¬å¼ã‚µã‚¤ãƒˆé¸å®šï¼‰...")
    api_start = time.time()
    
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-lite",
            temperature=0,
            google_api_key=settings.GOOGLE_API_KEY,
            max_retries=0,
        ).with_structured_output(URLScoreList)
        resp: URLScoreList = _invoke_with_retry(
            llm,
            prompt.format(company=state.company, location=state.location, web_context=web_context),
        )
        api_elapsed = time.time() - api_start
        logger.info(f"  âœ… APIå‘¼ã³å‡ºã—æˆåŠŸ ({api_elapsed:.2f}ç§’)")
    except Exception as e:
        api_elapsed = time.time() - api_start
        logger.error(f"  âŒ APIå‘¼ã³å‡ºã—å¤±æ•— ({api_elapsed:.2f}ç§’)")
        logger.error(f"  ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
        raise
    
    sorted_urls = sorted(resp.urls, key=lambda x: x.score, reverse=True)
    logger.info("  ğŸ“Š URLã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°çµæœ:")
    for i, url_score in enumerate(sorted_urls[:5], 1):
        logger.info(f"     {i}. {url_score.url} (ã‚¹ã‚³ã‚¢: {url_score.score})")

    def _is_excluded(url: str) -> bool:
        # EXCLUDE_DOMAINS ã¯æ”¹è¡ŒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ— or ãƒªã‚¹ãƒˆã‚’æƒ³å®š
        raw = settings.EXCLUDE_DOMAINS
        domains = [s.strip() for s in raw.splitlines() if s.strip()]
        return any(domain in url for domain in domains)

    state.urls = [url.url for url in sorted_urls if not _is_excluded(url.url)]
    
    node_elapsed = time.time() - node_start
    logger.info(f"  âœ… é¸å®šã•ã‚ŒãŸURL: {len(state.urls)}å€‹")
    logger.info(f"  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: {node_elapsed:.2f}ç§’")

    return state


def node_fetch_html(state: ExtractState) -> ExtractState:
    """é¸å®šURLã®HTMLã‚’å–å¾—ã—çŠ¶æ…‹ã«æ ¼ç´ã™ã‚‹.

    Args:
        state: `selected_url` ã‚’å«ã‚€çŠ¶æ…‹ã€‚

    Returns:
        dict: `html` ã‚’è¿½åŠ ã—ãŸçŠ¶æ…‹ï¼ˆå¤±æ•—æ™‚ã¯ç©ºæ–‡å­—ï¼‰ã€‚

    """
    node_start = time.time()
    logger.info("-" * 60)
    logger.info("[NODE 3/3] node_fetch_html - ä¼šç¤¾æƒ…å ±æŠ½å‡º")
    
    # URLå€™è£œãŒç„¡ã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹ï¼ˆValidationErrorã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
    if not state.urls:
        logger.warning("  âš ï¸ URLå€™è£œãŒ0å€‹ - æŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        raise ValueError("URLå€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ä¼šç¤¾æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã€‚")
    
    url = state.urls.pop(0)
    logger.info(f"  å¯¾è±¡URL: {url}")
    
    logger.info("  ğŸ•·ï¸ Webãƒšãƒ¼ã‚¸ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­ï¼ˆdepth=1, timeout=30ç§’ï¼‰...")
    crawl_start = time.time()
    web_context = crawl_markdown(url, depth=1, timeout=30)
    crawl_elapsed = time.time() - crawl_start
    if not web_context:
        logger.warning(f"  âš ï¸ ã‚¯ãƒ­ãƒ¼ãƒ«å¤±æ•—ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({crawl_elapsed:.2f}ç§’)")
        raise ValueError(f"URL {url} ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ï¼‰ã€‚")
    logger.info(f"  âœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº† ({crawl_elapsed:.2f}ç§’, {len(web_context)}æ–‡å­—)")

    prompt = load_prompt(str(BASE_DIR / "agent/prompts/extract_contact.yaml"), encoding="utf-8")
    logger.debug("  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ãƒ¼ãƒ‰å®Œäº†")
    
    logger.info("  ğŸ¤– Gemini APIå‘¼ã³å‡ºã—ä¸­ï¼ˆä¼šç¤¾æƒ…å ±æŠ½å‡ºï¼‰...")
    logger.info(f"     å¿…é ˆæ¥­ç¨®: {state.required_businesses}")
    logger.info(f"     å¿…é ˆã‚¸ãƒ£ãƒ³ãƒ«: {state.required_genre}")
    api_start = time.time()
    
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-lite",
            temperature=0,
            google_api_key=settings.GOOGLE_API_KEY,
            max_retries=0,
        ).with_structured_output(LLMCompanyInfo)
        resp: LLMCompanyInfo = _invoke_with_retry(
            llm,
            prompt.format(
                required_businesses=state.required_businesses,
                required_genre=state.required_genre,
                web_context=web_context,
            ),
        )
        api_elapsed = time.time() - api_start
        logger.info(f"  âœ… APIå‘¼ã³å‡ºã—æˆåŠŸ ({api_elapsed:.2f}ç§’)")
        logger.info("  ğŸ“‹ æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±:")
        logger.info(f"     ä¼šç¤¾å: {resp.company}")
        logger.info(f"     é›»è©±ç•ªå·: {resp.tel}")
        logger.info(f"     ä½æ‰€: {resp.address}")
        logger.info(f"     URL: {resp.url}")
        logger.info(f"     ãŠå•ã„åˆã‚ã›URL: {resp.contact_url}")
    except Exception as e:
        api_elapsed = time.time() - api_start
        logger.error(f"  âŒ APIå‘¼ã³å‡ºã—å¤±æ•— ({api_elapsed:.2f}ç§’)")
        logger.error(f"  ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:200]}")
        raise

    state.company_info = resp.model_dump()
    
    node_elapsed = time.time() - node_start
    logger.info(f"  â±ï¸ ãƒãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: {node_elapsed:.2f}ç§’")
    
    return state
