import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import tldextract
import logging
import sys
import signal
import time
import re
import urllib.parse
import threading
import urllib3
import chardet
import ssl
import json
import os
import argparse
from requests.exceptions import RequestException, SSLError, ConnectTimeout, ReadTimeout

# SSLè­¦å‘Šã‚’ç„¡åŠ¹åŒ–
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def setup_logging():
    """ãƒ­ã‚®ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®š"""
    logger = logging.getLogger('contact_finder')
    logger.setLevel(logging.INFO)
    
    file_handler = logging.FileHandler('contact_finder.log')
    file_handler.setLevel(logging.DEBUG)
    
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

logger = setup_logging()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

TIMEOUT = 15
DELAY = 1

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ•ãƒ©ã‚°
stop_requested = False
original_encoding = None

# é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
PROGRESS_FILE = 'contact_finder_progress.json'

# é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆï¼ˆã‚³ãƒ¼ãƒ‰å†…ç›´æ¥æŒ‡å®šï¼‰
EXCLUDE_DOMAINS = [
    'suumo.jp',
    'tabelog.com',
    'indeed.com',
    'xn--pckua2a7gp15o89zb.com'
    # å¿…è¦ã«å¿œã˜ã¦è¿½åŠ ã—ã¦ãã ã•ã„
]

def is_excluded_domain(url, exclude_domains):
    """é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¤å®šï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰"""
    try:
        if not exclude_domains:
            return False
        
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        
        for exclude_domain in exclude_domains:
            if exclude_domain.lower() in domain:
                logger.debug(f"é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³æ¤œå‡º: {url} (é™¤å¤–å¯¾è±¡: {exclude_domain})")
                return True
        
        return False
    except Exception as e:
        logger.debug(f"é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
        return False

def detect_encoding(file_path):
    """æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡º"""
    try:
        with open(file_path, 'rb') as f:
            rawdata = f.read(100000)
        result = chardet.detect(rawdata)
        encoding = result['encoding']
        confidence = result['confidence']
        logger.info(f"æ¤œå‡ºã•ã‚ŒãŸæ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {encoding} (ä¿¡é ¼åº¦: {confidence:.2f})")
        return encoding
    except Exception as e:
        logger.warning(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºå¤±æ•—: {e}")
        return 'utf-8'

def normalize_url(url):
    """URLæ­£è¦åŒ–ï¼ˆãƒ—ãƒ­ãƒˆã‚³ãƒ«è‡ªå‹•è¿½åŠ ï¼‰"""
    if not url or not isinstance(url, str):
        return None
    
    url = str(url).strip()
    if not url or url.lower() == 'nan':
        return None
    
    if url.startswith(('http://', 'https://')):
        return url
    
    return 'https://' + url

def create_session():
    """ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½å‰Šé™¤ï¼‰"""
    session = requests.Session()
    return session

def safe_request(url, **kwargs):
    """ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆå®‰å…¨ãªHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆSSL/TLS + HTTPãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰"""
    session = create_session()
    
    # SSL/TLSè¨­å®šã‚’æ®µéšçš„ã«è©¦è¡Œ
    ssl_configs = [
        {'verify': True},
        {'verify': False},
        {'verify': False, 'allow_redirects': True}
    ]
    
    for ssl_config in ssl_configs:
        try:
            kwargs.update(ssl_config)
            response = session.get(url, **kwargs)
            return response
        except SSLError as ssl_error:
            logger.warning(f"SSLè¨­å®š {ssl_config} ã§å¤±æ•—: {url}")
            continue
        except (ConnectTimeout, ReadTimeout) as timeout_error:
            logger.warning(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({ssl_config}): {url}")
            continue
        except Exception as e:
            logger.debug(f"ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ ({ssl_config}): {url} - {str(e)}")
            continue
    
    # HTTPãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
    if url.startswith('https://'):
        http_url = url.replace('https://', 'http://')
        try:
            logger.info(f"HTTPãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©¦è¡Œ: {http_url}")
            kwargs['verify'] = False
            response = session.get(http_url, **kwargs)
            return response
        except Exception as e:
            logger.error(f"HTTPãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—: {http_url} - {str(e)}")
    
    raise Exception(f"å…¨ã¦ã®SSL/HTTPæ–¹æ³•ã§å¤±æ•—: {url}")

def contains_mailto_link(soup, base_url):
    """mailto:ãƒªãƒ³ã‚¯æ¤œå‡º"""
    mailto_links = []
    for a in soup.find_all('a', href=True):
        href = a['href'].lower().strip()
        if href.startswith('mailto:'):
            text = a.get_text(strip=True)
            mailto_links.append({'href': href, 'text': text})
            logger.debug(f"mailto link found: {href} (text: '{text}')")
    
    if mailto_links:
        logger.info(f"ğŸ“§ mailto:ãƒªãƒ³ã‚¯æ¤œå‡º: {len(mailto_links)}ä»¶ ({base_url})")
        return True
    
    return False

# é«˜ç²¾åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
SCORE_CRITERIA = {
    'url_exact_match': 500,
    'text_exact_match': 300,
    'nav_bonus': 200,
    'footer_bonus': 150,
    'url_partial_match': 100,
    'text_partial_match': 60,
    'contact_section': 250,
    'icon_bonus': 100,
    'company_contact_bonus': 200,
    'fragment_bonus': 300,
    'image_alt_bonus': 200,
    'external_form_bonus': 300,
    'encoded_path_bonus': 400,
    'subdirectory_bonus': 150,
    'error_page_bonus': 100,
    'area_map_bonus': 250,
    'external_trusted_bonus': 350,
    'navigation_menu_bonus': 180,
    'ul_list_bonus': 120
}

# æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
URL_PATTERNS = [
    'contact', 'contacts', 'inquiry', 'inquiries', 'enquiry', 'enquiries',
    'ãŠå•ã„åˆã‚ã›', 'å•ã„åˆã‚ã›', 'ã‚³ãƒ³ã‚¿ã‚¯ãƒˆ', 'contact-us', 'contactus',
    'form', 'support', 'help', 'feedback', 'quest', 'toiawase',
    'publics/index', 'pages', 'mailform', 'preference-form', 'reservation',
    'otoiawase', 'soudan', 'soudankai', 'request', 'application',
    'order', 'estimate', 'mitsumori', 'reserve', 'ask', 'entry',
    'smarts/index', 'detail', 'inq', 'mail', 'cgi-bin/contact',
    'contact.html', 'inquiry.html', 'form.html', 'toiawase.html',
    'publics/index/5', 'pages/3', 'pages/5', 'mail.html',
    'company', 'about', 'corp', 'corporate', 'info', 'information',
    'otoiawase_koujyukai.html'
]

TEXT_PATTERNS = [
    'ãŠå•ã„åˆã‚ã›', 'ãŠå•åˆã›', 'å•ã„åˆã‚ã›', 'ãŠå•ã„åˆã‚ã›ã¯ã“ã¡ã‚‰',
    'ã‚³ãƒ³ã‚¿ã‚¯ãƒˆ', 'ã”é€£çµ¡', 'ã”ç›¸è«‡', 'ãŠæ°—è»½ã«', 'ã‚µãƒãƒ¼ãƒˆ',
    'ãƒ˜ãƒ«ãƒ—', 'ãƒ•ã‚©ãƒ¼ãƒ ', 'ãƒ¡ãƒ¼ãƒ«ãƒ•ã‚©ãƒ¼ãƒ ', 'ãŠå®¢æ§˜çª“å£', 'å–¶æ¥­çª“å£',
    'è³‡æ–™è«‹æ±‚', 'è¦‹ç©ã‚‚ã‚Š', 'ãŠè¦‹ç©ã‚Š', 'äºˆç´„', 'ã”äºˆç´„',
    'contact', 'contact us', 'inquiry', 'enquiry', 'get in touch',
    'reach us', 'support', 'help', 'feedback', 'ask', 'request',
    'ãŠå•ã„åˆã‚ã›å…ˆ', 'ã‚¢ã‚¯ã‚»ã‚¹ï¼†ãŠå•ã„åˆã‚ã›'
]

CONTACT_ICON_PATTERNS = [
    'fa-phone', 'fa-envelope', 'fa-mail', 'fa-contact',
    'icon-phone', 'icon-mail', 'icon-contact',
    'phone', 'mail', 'contact', 'envelope',
    'fas fa-envelope'
]

FRAGMENT_PATTERNS = ['#contact', '#inq', '#inquiry', '#form', '#toiawase', '#section08', '#section7']

INVALID_PATTERNS = [
    r'javascript:\s*void\s*\(\s*0?\s*\)',
    r'javascript:\s*;',
    r'^#[\w]*$',
    r'mailto:',
    r'tel:',
    r'javascript:void\(\d+\)',
    r'javascript:\s*return\s+false',
    r'window\.location',
    r'^\s*$'
]

EXCLUDE_PATTERNS = [
    'contact-lens', 'contract', 'ã‚µãƒ¼ãƒ“ã‚¹ç›¸è«‡', 'åˆ©ç”¨ç›¸è«‡',
    'æ¡ç”¨', 'recruit', 'æ±‚äºº', 'privacy',
    'ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼', 'åˆ©ç”¨è¦ç´„', 'terms'
]

def normalize_encoded_url(url):
    """URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ­£è¦åŒ–"""
    try:
        return urllib.parse.unquote(url, encoding='utf-8')
    except:
        return url

def normalize_domain(url):
    """ãƒ‰ãƒ¡ã‚¤ãƒ³æ­£è¦åŒ–ï¼ˆwww.çµ±ä¸€ï¼‰"""
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        if domain.startswith('www.'):
            domain = domain[4:]
        return domain
    except:
        return ''

def is_contact_external_domain(url, text):
    """æ‹¡å¼µç‰ˆå¤–éƒ¨ãƒ•ã‚©ãƒ¼ãƒ ã‚µãƒ¼ãƒ“ã‚¹è¨±å¯åˆ¤å®š"""
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        path = parsed.path.lower()
        
        trusted_domains = [
            'secure.multi.ne.jp',
            'ssl.formman.com',
            'form.run',
            'mailform.org',
            'ws.formzu.net',
            'ssl.form-mailer.jp',
            'ssl.xaas3.jp',
            'xaas3.jp',
            'samidare.jp',
            's8377681',
            'm5523948'
        ]
        
        if any(trusted_domain in domain for trusted_domain in trusted_domains):
            contact_indicators = ['inquiry', 'contact', 'form', 'mail', 'inquiryedit']
            if any(indicator in path for indicator in contact_indicators) or \
               any(pattern in text.lower() for pattern in TEXT_PATTERNS):
                logger.debug(f"ä¿¡é ¼å¤–éƒ¨ãƒ•ã‚©ãƒ¼ãƒ ã‚’è¨±å¯: {url}")
                return True
    except:
        pass
    
    return False

def is_same_domain_enhanced(base_url, extracted_url):
    """å¼·åŒ–ç‰ˆãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯"""
    try:
        base_domain = normalize_domain(base_url)
        extracted_domain = normalize_domain(extracted_url)
        return base_domain == extracted_domain and base_domain != ''
    except:
        return False

def extract_clean_text(element):
    """ã‚¯ãƒªãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
    if not element:
        return ''
    
    element_copy = element.__copy__()
    
    for icon in element_copy.find_all(['i', 'span'], class_=True):
        icon_classes = ' '.join(icon.get('class', []))
        if any(pattern in icon_classes.lower() for pattern in ['fa-', 'icon-', 'glyphicon']):
            icon.decompose()
    
    text = element_copy.get_text(strip=True)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def extract_image_alt_text(element):
    """ç”»åƒaltãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
    alt_texts = []
    images = element.find_all('img')
    for img in images:
        alt = img.get('alt', '').strip()
        if alt:
            alt_texts.append(alt)
    return ' '.join(alt_texts)

def detect_contact_icons(element):
    """ã‚³ãƒ³ã‚¿ã‚¯ãƒˆã‚¢ã‚¤ã‚³ãƒ³æ¤œå‡º"""
    if not element:
        return False
    
    icons = element.find_all(['i', 'span'], class_=True)
    for icon in icons:
        icon_classes = ' '.join(icon.get('class', []))
        if any(pattern in icon_classes.lower() for pattern in CONTACT_ICON_PATTERNS):
            return True
    
    return False

def find_image_map_links(soup, base_url):
    """ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒãƒƒãƒ—ï¼ˆ<area>ã‚¿ã‚°ï¼‰æ¤œå‡º"""
    area_links = []
    
    for area in soup.find_all('area', href=True):
        href = area['href'].strip()
        coords = area.get('coords', '')
        shape = area.get('shape', '')
        absolute_url = urljoin(base_url, href)
        area_links.append((absolute_url, f'area-map-{shape}', area))
        logger.debug(f"ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒãƒƒãƒ—æ¤œå‡º: {absolute_url} (coords: {coords})")
    
    if area_links:
        logger.info(f"ğŸ—ºï¸ ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒãƒƒãƒ—ãƒªãƒ³ã‚¯æ¤œå‡º: {len(area_links)}ä»¶")
    
    return area_links

def find_navigation_menu_links(soup, base_url, exclude_domains=None):
    """ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼å°‚ç”¨æ¤œç´¢ï¼ˆul/liå¼·åŒ–å¯¾å¿œï¼‰"""
    nav_links = []
    
    # 1. ID/ã‚¯ãƒ©ã‚¹ã«nav, menu, navigationã‚’å«ã‚€è¦ç´ 
    nav_indicators = ['nav', 'menu', 'navigation', 'gnav', 'header-menu', 'mainmenu']
    
    for indicator in nav_indicators:
        # IDæ¤œç´¢
        elements = soup.find_all(attrs={'id': re.compile(indicator, re.I)})
        for element in elements:
            links = extract_links_from_section(element, base_url, exclude_domains)
            if links:
                nav_links.extend(links)
                logger.debug(f"ãƒŠãƒ“ID({indicator})ã§ãƒªãƒ³ã‚¯æ¤œå‡º: {len(links)}ä»¶")
        
        # ã‚¯ãƒ©ã‚¹æ¤œç´¢
        elements = soup.find_all(class_=re.compile(indicator, re.I))
        for element in elements:
            links = extract_links_from_section(element, base_url, exclude_domains)
            if links:
                nav_links.extend(links)
                logger.debug(f"ãƒŠãƒ“ã‚¯ãƒ©ã‚¹({indicator})ã§ãƒªãƒ³ã‚¯æ¤œå‡º: {len(links)}ä»¶")
    
    # 2. navimenuç³»IDã®æ¤œç´¢ï¼ˆå…·ä½“ä¾‹å¯¾å¿œï¼‰
    navimenu_elements = soup.find_all(attrs={'id': re.compile(r'navimenu|navi.*menu|menu.*navi', re.I)})
    for element in navimenu_elements:
        element_id = element.get('id', '')
        
        # è¦ªè¦ç´ ã‚‚å«ã‚ã¦æ¤œç´¢
        parent = element.find_parent()
        if parent:
            links = extract_links_from_section(parent, base_url, exclude_domains)
            if links:
                nav_links.extend(links)
                logger.debug(f"ãƒŠãƒ“è¦ªè¦ç´ ({element_id})ã§ãƒªãƒ³ã‚¯æ¤œå‡º: {len(links)}ä»¶")
        
        # è¦ç´ è‡ªä½“ã‚‚æ¤œç´¢
        links = extract_links_from_section(element, base_url, exclude_domains)
        if links:
            nav_links.extend(links)
            logger.debug(f"ãƒŠãƒ“è¦ç´ ({element_id})ã§ãƒªãƒ³ã‚¯æ¤œå‡º: {len(links)}ä»¶")
    
    # 3. æ•°å­—ä»˜ããƒŠãƒ“IDæ¤œç´¢ï¼ˆnavimenu1, navimenu2, ...ï¼‰
    for i in range(1, 21):  # navimenu1ã‹ã‚‰navimenu20ã¾ã§
        nav_id = f'navimenu{i}'
        element = soup.find(attrs={'id': nav_id})
        if element:
            # è¦ç´ è‡ªä½“ã‚’æ¤œç´¢
            links = extract_links_from_section(element, base_url, exclude_domains)
            if links:
                nav_links.extend(links)
                logger.debug(f"æ•°å­—ä»˜ããƒŠãƒ“({nav_id})ã§ãƒªãƒ³ã‚¯æ¤œå‡º: {len(links)}ä»¶")
            
            # è¦ªè¦ç´ ã‚‚æ¤œç´¢
            parent = element.find_parent()
            if parent:
                parent_links = extract_links_from_section(parent, base_url, exclude_domains)
                if parent_links:
                    nav_links.extend(parent_links)
                    logger.debug(f"æ•°å­—ä»˜ããƒŠãƒ“è¦ª({nav_id})ã§ãƒªãƒ³ã‚¯æ¤œå‡º: {len(parent_links)}ä»¶")
    
    if nav_links:
        logger.info(f"ğŸ§­ ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒªãƒ³ã‚¯æ¤œå‡º: {len(nav_links)}ä»¶")
    
    return nav_links

def find_ul_li_links(soup, base_url, exclude_domains=None):
    """UL/LI ã‚¿ã‚°å°‚ç”¨æ¤œç´¢"""
    ul_li_links = []
    
    # 1. ULã‚¿ã‚°æ¤œç´¢
    ul_elements = soup.find_all('ul')
    for ul in ul_elements:
        ul_class = ' '.join(ul.get('class', []))
        ul_id = ul.get('id', '')
        
        links = extract_links_from_section(ul, base_url, exclude_domains)
        if links:
            ul_li_links.extend(links)
            logger.debug(f"ULã‚¿ã‚°ã§ãƒªãƒ³ã‚¯æ¤œå‡º: {len(links)}ä»¶ (class: {ul_class}, id: {ul_id})")
    
    # 2. ç‹¬ç«‹LIã‚¿ã‚°æ¤œç´¢ï¼ˆè¦ªãŒULã§ãªã„å ´åˆï¼‰
    li_elements = soup.find_all('li')
    for li in li_elements:
        # è¦ªãŒULã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
        if li.find_parent('ul'):
            continue
        
        li_class = ' '.join(li.get('class', []))
        li_id = li.get('id', '')
        
        links = extract_links_from_section(li, base_url, exclude_domains)
        if links:
            ul_li_links.extend(links)
            logger.debug(f"ç‹¬ç«‹LIã‚¿ã‚°ã§ãƒªãƒ³ã‚¯æ¤œå‡º: {len(links)}ä»¶ (class: {li_class}, id: {li_id})")
    
    if ul_li_links:
        logger.info(f"ğŸ“‹ UL/LIãƒªãƒ³ã‚¯æ¤œå‡º: {len(ul_li_links)}ä»¶")
    
    return ul_li_links

def find_fragment_links(base_url, soup):
    """ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆãƒªãƒ³ã‚¯æ¤œå‡º"""
    fragment_candidates = []
    
    for fragment in FRAGMENT_PATTERNS:
        target_id = fragment[1:]  # '#'ã‚’é™¤å»
        section = soup.find(attrs={'id': target_id})
        if section:
            fragment_url = urljoin(base_url, fragment)
            fragment_candidates.append((fragment_url, f'section-{target_id}', section))
    
    return fragment_candidates

def is_valid_link_enhanced(href, text, element, base_url, exclude_domains=None):
    """å¼·åŒ–ç‰ˆãƒªãƒ³ã‚¯æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆé™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³å¯¾å¿œï¼‰"""
    # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å„ªå…ˆï¼‰
    if exclude_domains:
        absolute_url = urljoin(base_url, href)
        if is_excluded_domain(absolute_url, exclude_domains):
            logger.debug(f"é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—: {absolute_url}")
            return False
    
    # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹åˆ¥å‡¦ç†
    if '#' in href:
        fragment = href.split('#')[1].lower()
        if any(pattern[1:] in fragment for pattern in FRAGMENT_PATTERNS):
            return True
    
    # åŸºæœ¬çš„ãªç„¡åŠ¹ãƒªãƒ³ã‚¯ãƒã‚§ãƒƒã‚¯
    for pattern in INVALID_PATTERNS:
        if pattern != r'^#[\w]*$' or not any(frag[1:] in href for frag in FRAGMENT_PATTERNS):
            if re.search(pattern, href, re.IGNORECASE):
                return False
    
    clean_text = text.lower().strip()
    alt_text = extract_image_alt_text(element)
    combined_text = (clean_text + ' ' + alt_text.lower()).strip()
    
    normalized_href = normalize_encoded_url(href)
    
    # å¤–éƒ¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ç‰¹åˆ¥å‡¦ç†
    try:
        absolute_url = urljoin(base_url, href)
        if not is_same_domain_enhanced(base_url, absolute_url):
            if is_contact_external_domain(absolute_url, combined_text):
                return True
            return False
    except:
        return False
    
    # ã€ŒãŠå•ã„åˆã‚ã›å…ˆã€ã®ç‰¹åˆ¥å‡¦ç†
    if 'å•ã„åˆã‚ã›å…ˆ' in combined_text:
        exclude_urls = ['privacy', 'terms', 'policy']
        if any(pattern in normalized_href.lower() for pattern in exclude_urls):
            return False
        return True
    
    # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
    for pattern in EXCLUDE_PATTERNS:
        if pattern.lower() in combined_text:
            return False
    
    return True

def find_contact_section(soup):
    """ã‚³ãƒ³ã‚¿ã‚¯ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³æ¤œç´¢"""
    contact_sections = []
    
    for element in soup.find_all(['div', 'section', 'aside'],
                                attrs={'id': re.compile(r'contact', re.I)}):
        contact_sections.append(element)
    
    for element in soup.find_all(['div', 'section', 'aside'],
                                class_=re.compile(r'contact', re.I)):
        contact_sections.append(element)
    
    return contact_sections[0] if contact_sections else None

def extract_links_from_section(section, base_url, exclude_domains=None):
    """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯æŠ½å‡ºï¼ˆé™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³å¯¾å¿œï¼‰"""
    links = []
    
    for a in section.find_all('a', href=True):
        href = a['href'].strip()
        text = a.get_text(strip=True)
        
        if is_valid_link_enhanced(href, text, a, base_url, exclude_domains):
            absolute_url = urljoin(base_url, href)
            links.append((absolute_url, text, a))
    
    return links

def deep_contact_search_enhanced(soup, base_url, exclude_domains=None):
    """å®Œå…¨å¼·åŒ–ç‰ˆæ·±åº¦å„ªå…ˆæ¤œç´¢ï¼ˆé™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³å¯¾å¿œï¼‰"""
    contact_links = []
    
    # 1. ç›´æ¥ã‚³ãƒ³ã‚¿ã‚¯ãƒˆãƒªãƒ³ã‚¯æ¤œç´¢
    contact_links.extend(find_direct_contact_links_enhanced(soup, base_url, exclude_domains))
    
    # 2. ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒãƒƒãƒ—æ¤œç´¢
    image_map_links = find_image_map_links(soup, base_url)
    
    # ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒãƒƒãƒ—ãƒªãƒ³ã‚¯ã«ã‚‚é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯é©ç”¨
    filtered_image_map_links = []
    for url, text, element in image_map_links:
        if not exclude_domains or not is_excluded_domain(url, exclude_domains):
            filtered_image_map_links.append((url, text, element))
        else:
            logger.debug(f"é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ã‚ˆã‚Šã‚¤ãƒ¡ãƒ¼ã‚¸ãƒãƒƒãƒ—ãƒªãƒ³ã‚¯ã‚¹ã‚­ãƒƒãƒ—: {url}")
    
    contact_links.extend(filtered_image_map_links)
    
    # 3. ã‚³ãƒ³ã‚¿ã‚¯ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³æ¤œç´¢
    contact_section = find_contact_section(soup)
    if contact_section:
        section_links = extract_links_from_section(contact_section, base_url, exclude_domains)
        contact_links.extend(section_links)
    
    # 4. ãƒ•ãƒƒã‚¿ãƒ¼æ¤œç´¢
    footer = soup.find('footer')
    if footer:
        footer_links = extract_links_from_section(footer, base_url, exclude_domains)
        contact_links.extend(footer_links)
    
    # 5. ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³æ¤œç´¢ï¼ˆnav, headerï¼‰
    nav_elements = soup.find_all(['nav', 'header'])
    for nav in nav_elements:
        nav_links = extract_links_from_section(nav, base_url, exclude_domains)
        contact_links.extend(nav_links)
    
    # 6. ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼å°‚ç”¨æ¤œç´¢
    nav_menu_links = find_navigation_menu_links(soup, base_url, exclude_domains)
    contact_links.extend(nav_menu_links)
    
    # 7. UL/LI ã‚¿ã‚°å°‚ç”¨æ¤œç´¢
    ul_li_links = find_ul_li_links(soup, base_url, exclude_domains)
    contact_links.extend(ul_li_links)
    
    return contact_links

def find_direct_contact_links_enhanced(soup, base_url, exclude_domains=None):
    """ç›´æ¥ã‚³ãƒ³ã‚¿ã‚¯ãƒˆãƒªãƒ³ã‚¯æ¤œç´¢ï¼ˆé™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³å¯¾å¿œï¼‰"""
    links = []
    
    for a in soup.find_all('a', href=True):
        href = a['href'].strip()
        text = a.get_text(strip=True)
        
        if is_valid_link_enhanced(href, text, a, base_url, exclude_domains):
            absolute_url = urljoin(base_url, href)
            links.append((absolute_url, text, a))
    
    return links

def calculate_enhanced_score(href, text, element, base_url, is_error_page=False):
    """æ‹¡å¼µç‰ˆã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆãƒŠãƒ“/ul/liå¯¾å¿œå¼·åŒ–ï¼‰"""
    score = 0
    
    try:
        normalized_href = normalize_encoded_url(href)
        url_path = urlparse(urljoin(base_url, normalized_href)).path.lower()
        clean_text = extract_clean_text(element).lower()
        alt_text = extract_image_alt_text(element).lower()
        combined_text = (clean_text + ' ' + alt_text).strip()
        has_contact_icon = detect_contact_icons(element)
        
        # 404ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ãƒœãƒ¼ãƒŠã‚¹
        if is_error_page:
            score += SCORE_CRITERIA['error_page_bonus']
            logger.debug(f"Error page bonus: {href}")
        
        # ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒãƒƒãƒ—ï¼ˆ<area>ï¼‰ãƒœãƒ¼ãƒŠã‚¹
        if element.name == 'area':
            score += SCORE_CRITERIA['area_map_bonus']
            logger.debug(f"Image map area bonus: {href}")
        
        # å¤–éƒ¨ä¿¡é ¼ãƒ•ã‚©ãƒ¼ãƒ ãƒœãƒ¼ãƒŠã‚¹
        try:
            if not is_same_domain_enhanced(base_url, href) and is_contact_external_domain(href, combined_text):
                score += SCORE_CRITERIA['external_trusted_bonus']
                logger.debug(f"External trusted form bonus: {href}")
        except:
            pass
        
        # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…æ¤œå‡ºãƒœãƒ¼ãƒŠã‚¹
        base_path = urlparse(base_url).path.rstrip('/')
        if base_path and base_path in url_path:
            score += SCORE_CRITERIA['subdirectory_bonus']
            logger.debug(f"Subdirectory bonus: {href}")
        
        # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆä»˜ãURLã®ç‰¹åˆ¥å‡¦ç†
        if '#' in href:
            fragment = href.split('#')[1].lower()
            if any(pattern[1:] in fragment for pattern in FRAGMENT_PATTERNS):
                score += SCORE_CRITERIA['fragment_bonus']
                logger.debug(f"Fragment pattern bonus: {href}")
        
        # URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãŠå•ã„åˆã‚ã›ãƒ‘ã‚¹ã®æ¤œå‡º
        decoded_path = normalize_encoded_url(url_path)
        if 'ãŠå•ã„åˆã‚ã›' in decoded_path or 'ãŠå•åˆã›' in decoded_path:
            score += SCORE_CRITERIA['encoded_path_bonus']
            logger.debug(f"Encoded path bonus: {href}")
        
        # ç¢ºå®ŸãªãŠå•ã„åˆã‚ã›URLï¼ˆæœ€å„ªå…ˆï¼‰
        primary_contact_patterns = [
            'contact', 'contacts', 'inquiry', 'inquiries', 'ãŠå•ã„åˆã‚ã›',
            'å•ã„åˆã‚ã›', 'toiawase', 'form', 'mailform', 'mail.html', 'mailform',
            'otoiawase_koujyukai.html', 'inquiryedit'
        ]
        
        # ä¼šç¤¾æƒ…å ±ç³»URL
        company_patterns = ['company', 'about', 'corp', 'corporate', 'info']
        
        url_score = 0
        text_score = 0
        
        # URLåŸºæœ¬ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        for pattern in primary_contact_patterns:
            if pattern in decoded_path:
                if f'/{pattern}' in decoded_path or f'{pattern}/' in decoded_path or decoded_path.endswith(f'/{pattern}'):
                    url_score = SCORE_CRITERIA['url_exact_match']
                    logger.debug(f"URL exact match: {href} (pattern: {pattern})")
                    break
                else:
                    url_score = SCORE_CRITERIA['url_partial_match']
                    logger.debug(f"URL partial match: {href} (pattern: {pattern})")
        
        # ãƒ†ã‚­ã‚¹ãƒˆï¼‹altãƒ†ã‚­ã‚¹ãƒˆã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        contact_text_found = False
        for pattern in TEXT_PATTERNS:
            if pattern in combined_text:
                if pattern == combined_text or combined_text.startswith(pattern):
                    text_score = SCORE_CRITERIA['text_exact_match']
                    contact_text_found = True
                    logger.debug(f"Text exact match: {href} (pattern: {pattern}, text: '{combined_text}')")
                    break
                else:
                    text_score = SCORE_CRITERIA['text_partial_match']
                    contact_text_found = True
                    logger.debug(f"Text partial match: {href} (pattern: {pattern}, text: '{combined_text}')")
        
        # ç”»åƒaltãƒ†ã‚­ã‚¹ãƒˆç‰¹åˆ¥ãƒœãƒ¼ãƒŠã‚¹
        if alt_text and any(pattern in alt_text for pattern in ['ãŠå•ã„åˆã‚ã›', 'ãŠå•åˆã›', 'å•ã„åˆã‚ã›', 'contact']):
            score += SCORE_CRITERIA['image_alt_bonus']
            logger.debug(f"Image alt text bonus: {href} (alt: {alt_text})")
        
        # ä¼šç¤¾æƒ…å ±ç³»URLã®ç‰¹åˆ¥å‡¦ç†
        is_company_url = any(pattern in decoded_path for pattern in company_patterns)
        if is_company_url and contact_text_found and has_contact_icon:
            score += text_score + SCORE_CRITERIA['company_contact_bonus']
            logger.debug(f"Company URL with contact indicators: {href}")
        else:
            score += url_score + text_score
        
        # ã‚¢ã‚¤ã‚³ãƒ³ãƒœãƒ¼ãƒŠã‚¹
        if has_contact_icon and contact_text_found:
            score += SCORE_CRITERIA['icon_bonus']
            logger.debug(f"Icon bonus: {href}")
        
        # ä½ç½®ã«ã‚ˆã‚‹åŠ ç‚¹ï¼ˆul/li/ãƒŠãƒ“å¯¾å¿œå¼·åŒ–ï¼‰
        parent = element.find_parent() if element else None
        context_bonus = 0
        
        for level in range(5):
            if not parent or not hasattr(parent, 'get'):
                break
                
            parent_class = ' '.join(parent.get('class', []))
            parent_id = parent.get('id', '')
            parent_tag = parent.name if hasattr(parent, 'name') else ''
            combined = (parent_class + parent_id + parent_tag).lower()
            
            # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ç³»ãƒœãƒ¼ãƒŠã‚¹
            if any(keyword in combined for keyword in ['nav', 'navigation', 'menu', 'header', 'gnav']):
                context_bonus = max(context_bonus, SCORE_CRITERIA['navigation_menu_bonus'])
                logger.debug(f"Navigation menu bonus: {href} (level: {level})")
            
            # ãƒ•ãƒƒã‚¿ãƒ¼ç³»ãƒœãƒ¼ãƒŠã‚¹
            elif 'footer' in combined:
                context_bonus = max(context_bonus, SCORE_CRITERIA['footer_bonus'])
                logger.debug(f"Footer bonus: {href} (level: {level})")
            
            # ã‚³ãƒ³ã‚¿ã‚¯ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒœãƒ¼ãƒŠã‚¹
            elif 'contact' in combined:
                context_bonus = max(context_bonus, SCORE_CRITERIA['contact_section'])
                logger.debug(f"Contact section bonus: {href} (level: {level})")
            
            # ul/liãƒœãƒ¼ãƒŠã‚¹
            elif parent_tag in ['ul', 'li']:
                context_bonus = max(context_bonus, SCORE_CRITERIA['ul_list_bonus'])
                logger.debug(f"UL/LI list bonus: {href} (level: {level}, tag: {parent_tag})")
            
            parent = parent.find_parent()
        
        score += context_bonus
        
        # å®Œå…¨é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_in_url = [
            'about-us', 'company-profile', 'corporate-info',
            'history', 'profile', 'ä¼šç¤¾æ¦‚è¦', 'ä¼æ¥­æƒ…å ±'
        ]
        
        if any(pattern in decoded_path for pattern in exclude_in_url):
            score = 0
            logger.debug(f"Excluded company info page: {href}")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã«ã‚ˆã‚‹é™¤å¤–
        exclude_texts = [
            'ä¼šç¤¾æ¦‚è¦', 'ä¼æ¥­æƒ…å ±', 'ä¼šç¤¾æ¡ˆå†…', 'about us',
            'company profile', 'corporate information'
        ]
        
        if any(pattern in combined_text for pattern in exclude_texts):
            score = max(0, score - 300)
            logger.debug(f"Text-based exclusion penalty: {href} (text: '{combined_text}')")
        
        # æœ€çµ‚ã‚¹ã‚³ã‚¢ã®ãƒ­ã‚°å‡ºåŠ›
        if score > 0:
            logger.debug(f"Final score: {href} = {score} points")
    
    except Exception as e:
        logger.debug(f"Error calculating enhanced score: {e}")
    
    return score

def search_subdirectory_paths(base_url):
    """ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå…±é€šãƒ‘ã‚¹æ¤œç´¢"""
    global stop_requested
    
    if stop_requested:
        return None
    
    try:
        parsed_base = urlparse(base_url)
        base_path = parsed_base.path.rstrip('/')
        
        subdirectory_paths = [
            '/contact', '/contacts', '/inquiry', '/form', '/mailform',
            '/contact.html', '/inquiry.html', '/form.html', '/index.php',
            '/mailform/index.php', '/contact/index.html', '/inquiry/index.html',
            '/mail.html', '/mailform/otoiawase_koujyukai.html'
        ]
        
        for path in subdirectory_paths:
            if stop_requested:
                return None
            
            try:
                test_url = f"{parsed_base.scheme}://{parsed_base.netloc}{base_path}{path}"
                test_response = safe_request(test_url, headers=HEADERS, timeout=5)
                
                if test_response.status_code == 200:
                    logger.info(f"ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹æ¤œå‡º: {test_url}")
                    return test_url
            except:
                continue
        
        return None
    
    except Exception as e:
        logger.debug(f"Error in subdirectory path search: {e}")
        return None

def fallback_search(base_url):
    """è¦ªãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½"""
    global stop_requested
    
    try:
        if stop_requested:
            return None
        
        parsed_url = urlparse(base_url)
        parent_domain = f"{parsed_url.scheme}://{parsed_url.netloc}/"
        
        if parent_domain != base_url:
            logger.info(f"è¦ªãƒ‰ãƒ¡ã‚¤ãƒ³ã§å†æ¤œç´¢: {parent_domain}")
            return find_contact_url_enhanced(parent_domain)
    except:
        pass
    
    return None

def print_progress(current, total, url, status):
    """é€²æ—è¡¨ç¤º"""
    progress = (current / total) * 100
    print(f"\rå‡¦ç†ä¸­ [{current}/{total}] {progress:.1f}% | {url} â†’ {status}", end="")

def handle_dynamic_content(url):
    """å¼·åŒ–ç‰ˆå‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯¾å¿œï¼ˆSeleniumå®‰å®šæ€§å‘ä¸Šï¼‰"""
    global stop_requested
    
    try:
        if stop_requested:
            return None
        
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        from selenium.webdriver.common.by import By
        from selenium.common.exceptions import TimeoutException, WebDriverException
        
        options = Options()
        options.headless = True
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-features=VizDisplayCompositor')
        options.add_argument('--remote-debugging-port=9222')
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-plugins')
        options.add_argument('--disable-images')
        
        driver = None
        try:
            driver = webdriver.Chrome(options=options)
            driver.set_page_load_timeout(20)
            driver.implicitly_wait(5)
            
            driver.get(url)
            
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, 'body'))
                )
            except TimeoutException:
                logger.warning(f"Selenium timeout, but proceeding: {url}")
                time.sleep(3)
            
            page_source = driver.page_source
            logger.debug(f"Seleniumã§ãƒšãƒ¼ã‚¸ã‚½ãƒ¼ã‚¹å–å¾—æˆåŠŸ: {url}")
            return page_source
        
        except WebDriverException as e:
            logger.error(f"Selenium WebDriverException: {url} - {str(e)}")
            return None
        except Exception as e:
            logger.error(f"Selenium unexpected error: {url} - {str(e)}")
            return None
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
    
    except ImportError:
        logger.warning("SeleniumãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚é€šå¸¸ã®requestsã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return None
    except Exception as e:
        if not stop_requested:
            logger.error(f"Selenium setup error: {e}")
        return None

def find_contact_url_enhanced(base_url, exclude_domains=None):
    """ul/liå¼·åŒ–ç‰ˆï¼šé™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³å¯¾å¿œç‰ˆ"""
    global stop_requested
    
    try:
        if stop_requested:
            return None
        
        time.sleep(DELAY)
        
        # ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆå®‰å…¨ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        response = safe_request(base_url, headers=HEADERS, timeout=TIMEOUT)
        
        is_error_page = False
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰æ‹¡å¼µå¯¾å¿œï¼ˆ403ã‚‚è§£æå¯¾è±¡ï¼‰
        if response.status_code == 200:
            logger.debug(f"æ­£å¸¸ã‚¢ã‚¯ã‚»ã‚¹: {base_url}")
        elif response.status_code == 404:
            logger.info(f"404ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã‚’è§£æ: {base_url}")
            is_error_page = True
        elif response.status_code == 403:
            logger.info(f"403ã‚¨ãƒ©ãƒ¼ã ãŒè§£æã‚’ç¶™ç¶š: {base_url}")
            is_error_page = True
        elif response.status_code >= 400:
            logger.warning(f"ã‚¨ãƒ©ãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹{response.status_code}ã€è§£æã‚’ã‚¹ã‚­ãƒƒãƒ—: {base_url}")
            return None
        
        if stop_requested:
            return None
        
        # HTMLã‚µã‚¤ã‚ºåˆ¤å®šç·©å’Œï¼ˆ50æ–‡å­—ä»¥ä¸Šã§è§£æç¶šè¡Œï¼‰
        if not response.text or len(response.text) < 50:
            logger.warning(f"HTMLå–å¾—å¤±æ•—ã¾ãŸã¯ã‚µã‚¤ã‚ºä¸è¶³: {base_url}")
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³å¯¾å¿œç‰ˆå¾¹åº•æ¤œç´¢
        all_candidates = deep_contact_search_enhanced(soup, base_url, exclude_domains)
        fragment_candidates = find_fragment_links(base_url, soup)
        all_candidates.extend(fragment_candidates)
        
        # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ãƒ•ãƒ©ã‚°ä»˜ãï¼‰
        scored_candidates = []
        for url, text, element in all_candidates:
            if stop_requested:
                return None
            score = calculate_enhanced_score(url, text, element, base_url, is_error_page)
            if score > 0:
                scored_candidates.append((url, score, text))
        
        # ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ã®æ¤œç´¢
        for form in soup.find_all('form'):
            if stop_requested:
                return None
            
            action = form.get('action', '')
            if action and is_valid_link_enhanced(action, '', form, base_url, exclude_domains):
                try:
                    absolute_url = urljoin(base_url, action)
                    scored_candidates.append((absolute_url, 400, 'form'))
                    logger.debug(f"Form action found: {absolute_url}")
                except:
                    continue
        
        # 2. ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã§è¦‹ã¤ã‹ã£ãŸå ´åˆã¯å³åº§ã«è¿”ã™
        if scored_candidates:
            scored_candidates.sort(key=lambda x: x[1], reverse=True)
            best_candidate = scored_candidates[0]
            status_msg = "403ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸å†…ã§æ¤œå‡º" if response.status_code == 403 else \
                        "404ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸å†…ã§æ¤œå‡º" if is_error_page else "ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã§æ¤œå‡º"
            logger.info(f"{status_msg}: {best_candidate[0]} (score: {best_candidate[1]})")
            return best_candidate[0]
        
        # 3. ã‚³ãƒ³ã‚¿ã‚¯ãƒˆãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€mailto:ãƒªãƒ³ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
        if contains_mailto_link(soup, base_url):
            logger.info("ğŸ“§ é€šå¸¸ã®ã‚³ãƒ³ã‚¿ã‚¯ãƒˆãƒªãƒ³ã‚¯ã¯è¦‹ã¤ã‹ã‚‰ãªã„ãŒã€mailto:ãƒªãƒ³ã‚¯ã‚’æ¤œå‡º")
            return 'MAILTO_DETECTED'
        
        # 4. ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã¿å‹•çš„æ¤œç´¢
        if stop_requested:
            return None
        
        logger.info(f"ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã§é™çš„æ¤œç´¢å¤±æ•—ã€å‹•çš„æ¤œç´¢ã‚’å®Ÿè¡Œ: {base_url}")
        dynamic_html = handle_dynamic_content(base_url)
        
        if dynamic_html and not stop_requested:
            soup = BeautifulSoup(dynamic_html, 'html.parser')
            all_candidates = deep_contact_search_enhanced(soup, base_url, exclude_domains)
            fragment_candidates = find_fragment_links(base_url, soup)
            all_candidates.extend(fragment_candidates)
            
            scored_candidates = []
            for url, text, element in all_candidates:
                if stop_requested:
                    return None
                score = calculate_enhanced_score(url, text, element, base_url, is_error_page)
                if score > 0:
                    scored_candidates.append((url, score, text))
            
            if scored_candidates:
                scored_candidates.sort(key=lambda x: x[1], reverse=True)
                best_candidate = scored_candidates[0]
                logger.info(f"å‹•çš„æ¤œç´¢ã§æ¤œå‡º: {best_candidate[0]} (score: {best_candidate[1]})")
                return best_candidate[0]
            
            # å‹•çš„æ¤œç´¢ã§ã‚‚ã‚³ãƒ³ã‚¿ã‚¯ãƒˆãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€å†åº¦mailto:ãƒã‚§ãƒƒã‚¯
            if contains_mailto_link(soup, base_url):
                logger.info("ğŸ“§ å‹•çš„æ¤œç´¢ã§ã‚‚ã‚³ãƒ³ã‚¿ã‚¯ãƒˆãƒªãƒ³ã‚¯ã¯è¦‹ã¤ã‹ã‚‰ãªã„ãŒã€mailto:ãƒªãƒ³ã‚¯ã‚’æ¤œå‡º")
                return 'MAILTO_DETECTED'
        
        # 5. ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…±é€šãƒ‘ã‚¹æ¤œç´¢
        if stop_requested:
            return None
        
        logger.info(f"ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å…±é€šãƒ‘ã‚¹æ¤œç´¢: {base_url}")
        subdirectory_result = search_subdirectory_paths(base_url)
        if subdirectory_result:
            return subdirectory_result
        
        # 6. æœ€å¾Œã®æ‰‹æ®µï¼šè¦ªãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        logger.info(f"ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã§è¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚è¦ªãƒ‰ãƒ¡ã‚¤ãƒ³ã§æ¤œç´¢: {base_url}")
        return fallback_search(base_url)
    
    except Exception as e:
        if not stop_requested:
            logger.error(f"Error in enhanced processing {base_url}: {str(e)}")
        return None

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆä¸­æ–­æ™‚ã®ä¿å­˜ç”¨ï¼‰
current_df = None
output_file = None

# progress.jsonæ©Ÿèƒ½ç¾¤ï¼ˆå®‰å…¨ç‰ˆï¼‰

def load_all_progress():
    """å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®é€²æ—ã‚’èª­ã¿è¾¼ã¿"""
    try:
        if os.path.exists(PROGRESS_FILE):
            with open(PROGRESS_FILE, 'r') as f:
                return json.load(f)
        return {}
    except json.JSONDecodeError:
        logger.warning("é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã¾ã™")
        return {}
    except Exception as e:
        logger.warning(f"é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def save_progress_safe(csv_file, last_processed_index):
    """å®‰å…¨ç‰ˆé€²æ—ä¿å­˜ï¼ˆå‰Šé™¤æ©Ÿèƒ½ãªã—ï¼‰"""
    global original_encoding
    
    if current_df is not None:
        try:
            # CSVä¿å­˜
            encoding_to_use = original_encoding if original_encoding else 'utf-8'
            current_df.to_csv(csv_file, index=False, encoding=encoding_to_use)
            logger.debug(f"CSVä¿å­˜å®Œäº†ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {encoding_to_use}ï¼‰")
            
            # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®é€²æ—ã‚’èª­ã¿è¾¼ã¿
            all_progress = load_all_progress()
            
            # ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®é€²æ—ã‚’æ›´æ–°
            file_key = os.path.abspath(csv_file)
            all_progress[file_key] = last_processed_index
            
            # progress.jsonã«å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®é€²æ—ã‚’ä¿å­˜
            with open(PROGRESS_FILE, 'w') as f:
                json.dump(all_progress, f, indent=2)
            
            logger.debug(f"å®‰å…¨ç‰ˆé€²æ—ä¿å­˜å®Œäº†: {file_key} -> {last_processed_index}")
        
        except Exception as e:
            logger.error(f"é€²æ—ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def get_start_index_safe(csv_file, force_restart=False):
    """å®‰å…¨ç‰ˆé–‹å§‹è¡Œæ±ºå®š"""
    if force_restart:
        logger.info("--forceæŒ‡å®š: æœ€åˆã‹ã‚‰å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
        return 0
    
    try:
        all_progress = load_all_progress()
        
        if not all_progress:
            logger.info("é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æœ€åˆã‹ã‚‰å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
            return 0
        
        file_key = os.path.abspath(csv_file)
        
        if file_key in all_progress:
            last_processed_index = all_progress[file_key]
            start_index = last_processed_index + 1
            logger.info(f"ğŸ”„ {os.path.basename(csv_file)}ã®å‡¦ç†ã‚’æ¤œå‡ºã€‚è¡Œ{start_index}ã‹ã‚‰è‡ªå‹•å†é–‹ã—ã¾ã™")
            print(f"âœ… {os.path.basename(csv_file)}ã®å‰å›ã®ç¶šãã‹ã‚‰è‡ªå‹•å†é–‹ã—ã¾ã™ï¼ˆè¡Œ{start_index}ã‹ã‚‰ï¼‰")
            return start_index
        else:
            logger.info(f"æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«{os.path.basename(csv_file)}ã‚’æ¤œå‡ºã€‚æœ€åˆã‹ã‚‰å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
            return 0
    
    except Exception as e:
        logger.warning(f"é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}ã€‚æœ€åˆã‹ã‚‰å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
        return 0

def log_completion_safe(csv_file):
    """å®‰å…¨ç‰ˆå®Œäº†ãƒ­ã‚°ï¼ˆå‰Šé™¤ã›ãšè¨˜éŒ²ã®ã¿ï¼‰"""
    try:
        logger.info(f"âœ… {os.path.basename(csv_file)}ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        logger.info(f"ğŸ“‹ é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«({PROGRESS_FILE})ã¯ãã®ã¾ã¾ä¿æŒã•ã‚Œã¾ã™")
        logger.info(f"ğŸ’¡ æ‰‹å‹•å‰Šé™¤ã—ãŸã„å ´åˆã¯: --reset ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
    except Exception as e:
        logger.warning(f"å®Œäº†ãƒ­ã‚°ã‚¨ãƒ©ãƒ¼: {e}")

def show_progress_status():
    """ç¾åœ¨ã®é€²æ—çŠ¶æ³ã‚’è¡¨ç¤º"""
    try:
        all_progress = load_all_progress()
        
        if not all_progress:
            print("ğŸ“‹ é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ï¼šå…¨ã¦æ–°è¦å‡¦ç†ã§ã™")
            return
        
        print("ğŸ“‹ ç¾åœ¨ã®é€²æ—çŠ¶æ³:")
        for file_path, last_index in all_progress.items():
            file_name = os.path.basename(file_path)
            print(f" â€¢ {file_name}: è¡Œ{last_index}ã¾ã§å®Œäº†")
    
    except Exception as e:
        logger.warning(f"é€²æ—çŠ¶æ³è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")

def signal_handler(sig, frame):
    """Ctrl+Cãƒãƒ³ãƒ‰ãƒ©"""
    global stop_requested
    stop_requested = True
    print('\nâš ï¸ ä¸­æ–­è¦æ±‚ã‚’å—ä¿¡ã—ã¾ã—ãŸã€‚å®‰å…¨ã«åœæ­¢ä¸­...')

def main():
    
    # å€‹åˆ¥URLæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ï¼ˆæ–°è¦è¿½åŠ ï¼‰
    if len(sys.argv) == 3 and not sys.argv[1].endswith('.csv'):
        company_name = sys.argv[1]
        domain = sys.argv[2]
        base_url = f"https://{domain}"
        
        try:
            result = find_contact_url_enhanced(base_url, EXCLUDE_DOMAINS)
            if result and result != 'MAILTO_DETECTED':
                print(result)
            elif result == 'MAILTO_DETECTED':
                print('MAILTO_DETECTED')
        except Exception as e:
            pass  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä½•ã‚‚å‡ºåŠ›ã—ãªã„
        
        return
    
    
    """ã‚³ãƒ¼ãƒ‰å†…ç›´æ¥æŒ‡å®šæ–¹å¼ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    global current_df, output_file, stop_requested, original_encoding
    
    # å¼•æ•°ç„¡ã—å¯¾å¿œã®argparseè¨­å®š
    parser = argparse.ArgumentParser(description='ä¼æ¥­ãŠå•ã„åˆã‚ã›URLè‡ªå‹•æŠ½å‡ºãƒ„ãƒ¼ãƒ«ï¼ˆã‚³ãƒ¼ãƒ‰å†…ç›´æ¥æŒ‡å®šç‰ˆï¼‰')
    parser.add_argument('csv_file', nargs='?', help='å‡¦ç†å¯¾è±¡ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')  # nargs='?' ã§ optional ã«
    parser.add_argument('--force', action='store_true', help='æœ€åˆã‹ã‚‰å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆé€²æ—ç„¡è¦–ï¼‰')
    parser.add_argument('--reset', action='store_true', help='é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¦çµ‚äº†ã—ã¾ã™')
    parser.add_argument('--status', action='store_true', help='ç¾åœ¨ã®é€²æ—çŠ¶æ³ã‚’è¡¨ç¤ºã—ã¦çµ‚äº†ã—ã¾ã™')
    
    args = parser.parse_args()
    
    # --status ã‚³ãƒãƒ³ãƒ‰å‡¦ç†
    if args.status:
        show_progress_status()
        sys.exit(0)
    
    # --reset ã‚³ãƒãƒ³ãƒ‰å‡¦ç†
    if args.reset:
        if os.path.exists(PROGRESS_FILE):
            os.remove(PROGRESS_FILE)
            logger.info(f"é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«({PROGRESS_FILE})ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            print(f"âœ… é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«({PROGRESS_FILE})ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        else:
            print(f"âœ… é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«({PROGRESS_FILE})ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã§ã—ãŸ")
        sys.exit(0)
    
    # csv_file ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒã‚§ãƒƒã‚¯
    if not args.csv_file:
        print("å‡¦ç†å¯¾è±¡ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        print("ä¾‹: python3 contact_finder.py your_data.csv")
        sys.exit(1)
    
    csv_file = args.csv_file
    
    if not os.path.exists(csv_file):
        logger.error(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ« '{csv_file}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ« '{csv_file}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)
    
    try:
        # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ã®è¨­å®šï¼ˆã‚³ãƒ¼ãƒ‰å†…ç›´æ¥æŒ‡å®šï¼‰
        exclude_domains = EXCLUDE_DOMAINS
        
        if exclude_domains:
            logger.info(f"ğŸš« é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½æœ‰åŠ¹: {len(exclude_domains)}ä»¶ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’é™¤å¤–å¯¾è±¡ã¨ã—ã¾ã™")
            print(f"ğŸš« é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½æœ‰åŠ¹: {len(exclude_domains)}ä»¶ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’é™¤å¤–å¯¾è±¡ã¨ã—ã¾ã™")
            print(f"ğŸ“ é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³: {', '.join(exclude_domains)}")
        else:
            logger.info("é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½ç„¡åŠ¹: ã™ã¹ã¦ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å‡¦ç†å¯¾è±¡ã¨ã—ã¾ã™")
            print("é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½ç„¡åŠ¹: ã™ã¹ã¦ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å‡¦ç†å¯¾è±¡ã¨ã—ã¾ã™")
        
        # å®‰å…¨ç‰ˆè‡ªå‹•å†é–‹ãƒ­ã‚¸ãƒƒã‚¯
        start_index = get_start_index_safe(csv_file, args.force)
        
        # å…ƒã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡ºãƒ»ä¿å­˜
        original_encoding = detect_encoding(csv_file)
        
        # CSVèª­ã¿è¾¼ã¿ï¼ˆå…ƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä½¿ç”¨ï¼‰
        try:
            df = pd.read_csv(csv_file, encoding=original_encoding)
            logger.info(f"å…ƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§èª­ã¿è¾¼ã¿æˆåŠŸ: {original_encoding}")
        except UnicodeDecodeError:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
            fallback_encodings = ['shift-jis', 'cp932', 'euc-jp', 'iso-2022-jp', 'latin1']
            for fallback_encoding in fallback_encodings:
                try:
                    logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§å†è©¦è¡Œ: {fallback_encoding}")
                    df = pd.read_csv(csv_file, encoding=fallback_encoding)
                    original_encoding = fallback_encoding
                    logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§æˆåŠŸ: {original_encoding}")
                    break
                except:
                    continue
            else:
                logger.error("å…¨ã¦ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§å¤±æ•—")
                raise Exception("ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
        
        current_df = df
        output_file = csv_file
        
        logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ (ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {original_encoding})")
        
        # Fåˆ—ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹5ï¼‰ã®ç¢ºèª
        if len(df.columns) <= 5:
            logger.error("Fåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # AFåˆ—ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹31ï¼‰ã®ç¢ºèªãƒ»ä½œæˆ
        af_col_index = 31
        while len(df.columns) <= af_col_index:
            df[f'Column_{len(df.columns)}'] = ''
        
        total_rows = len(df)
        processed = 0
        skipped = 0
        found = 0
        mailto_detected = 0
        excluded_count = 0  # é™¤å¤–ã‚«ã‚¦ãƒ³ã‚¿è¿½åŠ 
        
        logger.info(f"ã‚³ãƒ¼ãƒ‰å†…ç›´æ¥æŒ‡å®šç‰ˆå‡¦ç†é–‹å§‹: åˆè¨ˆ{total_rows}è¡Œ (é–‹å§‹è¡Œ: {start_index})")
        
        # ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ«ãƒ¼ãƒ—
        for index, row in df.iterrows():
            # é–‹å§‹è¡Œã¾ã§ã‚¹ã‚­ãƒƒãƒ—
            if index < start_index:
                skipped += 1
                continue
            
            # ä¸­æ–­ãƒã‚§ãƒƒã‚¯
            if stop_requested:
                print("\nğŸ›‘ å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™...")
                save_progress_safe(csv_file, index - 1)
                return
            
            # å…ƒã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ã‚­ãƒƒãƒ—åˆ¤å®šã‚’ç¶­æŒï¼ˆæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜ï¼‰
            if pd.notna(df.iloc[index, af_col_index]) and str(df.iloc[index, af_col_index]).strip() != '':
                skipped += 1
                continue
            
            # URLå–å¾—ã¨æ­£è¦åŒ–
            raw_url = df.iloc[index, 5]  # Fåˆ—
            url = normalize_url(raw_url)
            
            if not url:
                df.iloc[index, af_col_index] = None
                processed += 1
                continue
            
            # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å„ªå…ˆï¼‰
            if exclude_domains and is_excluded_domain(url, exclude_domains):
                df.iloc[index, af_col_index] = 'é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³'
                excluded_count += 1
                processed += 1
                logger.info(f"ğŸš« é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—: {url}")
                continue
            
            # é€²æ—è¡¨ç¤º
            print_progress(index+1, total_rows, str(url), "å‡¦ç†ä¸­...")
            logger.info(f"å‡¦ç†ä¸­ ({index+1}/{total_rows}): {url}")
            
            # ä¸­æ–­ãƒã‚§ãƒƒã‚¯ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å‡¦ç†å‰ï¼‰
            if stop_requested:
                print("\nğŸ›‘ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å‡¦ç†å‰ã«ä¸­æ–­...")
                save_progress_safe(csv_file, index - 1)
                return
            
            # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³å¯¾å¿œç‰ˆæ¤œç´¢å®Ÿè¡Œ
            contact_url = find_contact_url_enhanced(str(url), exclude_domains)
            
            # ä¸­æ–­ãƒã‚§ãƒƒã‚¯ï¼ˆå‡¦ç†å¾Œï¼‰
            if stop_requested:
                print("\nğŸ›‘ å‡¦ç†å®Œäº†å¾Œã«ä¸­æ–­...")
                if contact_url == 'MAILTO_DETECTED':
                    df.iloc[index, af_col_index] = 'ãƒ¡ãƒ¼ãƒ©ãƒ¼èµ·å‹•ãƒªãƒ³ã‚¯ã®ãŸã‚æ¤œå‡ºä¸å¯'
                else:
                    df.iloc[index, af_col_index] = contact_url if contact_url else None
                save_progress_safe(csv_file, index)
                return
            
            # çµæœåˆ¤å®šã¨æ–‡è¨€è¨­å®š
            if contact_url == 'MAILTO_DETECTED':
                result = 'ãƒ¡ãƒ¼ãƒ©ãƒ¼èµ·å‹•ãƒªãƒ³ã‚¯ã®ãŸã‚æ¤œå‡ºä¸å¯'
                mailto_detected += 1
                logger.info(f"ğŸ“§ ãƒ¡ãƒ¼ãƒ©ãƒ¼èµ·å‹•ãƒªãƒ³ã‚¯ã®ãŸã‚æ¤œå‡ºä¸å¯: {url}")
            elif contact_url:
                result = contact_url
                found += 1
                logger.info(f"âœ… æ¤œå‡º: {result}")
            else:
                result = None
                logger.info(f"âŒ æ¤œå‡ºä¸å¯ï¼ˆç©ºæ¬„ã§å‡ºåŠ›ï¼‰")
            
            df.iloc[index, af_col_index] = result
            processed += 1
            
            # å®‰å…¨ç‰ˆéšæ™‚ä¿å­˜
            if processed % 10 == 0:
                save_progress_safe(csv_file, index)
                logger.info(f"é€²æ—ä¿å­˜: {processed}ä»¶å‡¦ç†ã€{found}ä»¶æ¤œå‡ºã€{mailto_detected}ä»¶mailtoã€{excluded_count}ä»¶é™¤å¤–ã€{skipped}ä»¶ã‚¹ã‚­ãƒƒãƒ—")
        
        # å®‰å…¨ç‰ˆæœ€çµ‚ä¿å­˜
        save_progress_safe(csv_file, total_rows - 1)
        
        # å®Œäº†ãƒ­ã‚°ï¼ˆå‰Šé™¤ã›ãšè¨˜éŒ²ã®ã¿ï¼‰
        log_completion_safe(csv_file)
        
        success_rate = (found / processed * 100) if processed > 0 else 0
        mailto_rate = (mailto_detected / processed * 100) if processed > 0 else 0
        exclude_rate = (excluded_count / processed * 100) if processed > 0 else 0
        
        logger.info(f"ğŸ‰ ã‚³ãƒ¼ãƒ‰å†…ç›´æ¥æŒ‡å®šç‰ˆå‡¦ç†å®Œäº†: {processed}ä»¶å‡¦ç†ã€{found}ä»¶æ¤œå‡ºï¼ˆæˆåŠŸç‡{success_rate:.1f}%ï¼‰ã€{mailto_detected}ä»¶mailtoï¼ˆ{mailto_rate:.1f}%ï¼‰ã€{excluded_count}ä»¶é™¤å¤–ï¼ˆ{exclude_rate:.1f}%ï¼‰ã€{skipped}ä»¶ã‚¹ã‚­ãƒƒãƒ—")
        
        print(f"\nğŸ‰ å‡¦ç†å®Œäº†ï¼")
        print(f"âœ… æ¤œå‡ºæˆåŠŸ: {found}ä»¶ ({success_rate:.1f}%)")
        print(f"ğŸ“§ ãƒ¡ãƒ¼ãƒ«æ¤œå‡º: {mailto_detected}ä»¶ ({mailto_rate:.1f}%)")
        print(f"ğŸš« é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³: {excluded_count}ä»¶ ({exclude_rate:.1f}%)")
        print(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {skipped}ä»¶")
        print(f"ğŸ“ é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³: {', '.join(exclude_domains) if exclude_domains else 'ãªã—'}")
    
    except KeyboardInterrupt:
        logger.info("\nğŸ›‘ KeyboardInterrupt ã§ä¸­æ–­...")
        save_progress_safe(csv_file, index if 'index' in locals() else -1)
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
        save_progress_safe(csv_file, index if 'index' in locals() else -1)

if __name__ == '__main__':
    # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®ç™»éŒ²
    signal.signal(signal.SIGINT, signal_handler)
    logger.info(f"ã‚³ãƒ¼ãƒ‰å†…ç›´æ¥æŒ‡å®šç‰ˆå‡¦ç†é–‹å§‹")
    main()
    logger.info("âœ¨ å…¨å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
